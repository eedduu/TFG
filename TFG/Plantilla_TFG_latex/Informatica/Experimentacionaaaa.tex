\section{Experimentación}

Con el objetivo de una correcta comparación de resultados entre las técnicas basadas en gradiente descendente y las metaheurísticas, se ha desarrollado una batería experimental que usa varios modelos, datasets, y tipos de problema, en los que se aplicarán tres técnicas basadas en descenso de gradiente y tres técnicas metaheurísticas. Las elecciones de situaciones en la batería experimental se han realizado intentando abarcar muchas situaciones de manera que se comparen las técnicas en ámbitos diversos para ver si se confirman los resultados recientes en todos ellos. En este sentido se ha intentado encontrar un equilibrio, seleccionando problemas de clasificación y regresión, datasets con amplia variedad en cuanto a la cantidad de datos de entrada, modelos más sencillos y simples como el MLP hasta algunos muy sofisticados y rozando el estado del arte como son las ResNet.

\subsection{Modelos}

Dentro de las redes neuronales, usaremos MLP y ConvNets. Tendremos 5 modelos de MLP: con 1, 2, 5, y 10 capas cada uno. El número de unidades ocultas de cada capa será una potencia de dos, ya que resulta más eficiente computacionalmente; y se organizarán de forma que al comienzo se vayan aumentando el número de unidades por capa, hasta llegar a la capa de la mitad de la red, y entonces vaya decreciendo el número de unidades por capa. Esta estructura es más común en la literatura, y ayuda a capturar las características de alto nivel de la red en la primera mitad creciente para luego refinarlas en la segunda mitad decreciente (TODO revisar esto con referencias). 

\begin{enumerate}
    \item 32 

    \item 32, 32

    \item 32, 64, 128, 64, 32

    \item 32, 64, 128, 256, 512, 512, 256, 128, 64, 32
\end{enumerate}

TODO poner numero de parámetros de cada MLP.

En cuanto a las ConvNets usaremos LeNet-5 y ResNet-18. No usaremos LeNet-5 en su versión original sino que modificaremos las funciones de activación sigmoides por ReLU, y las capas de average pool por maxpool, con el objetivo de mejorar la eficiencia y en consonancia con las tendencias de la literatura en estas decisiones. Usaremos concretamente el modelo ResNet-18 por cuestiones de poder de cómputo, ya que un modelo mayor requeriría demasiados recursos computacionales. Igualmente un modelo menor perdería de cierta manera el objetivo de esta familia de modelos, que es aprender la función identidad a través de residuos entre bloques convolucionales, por lo que si la red es demasiado pequeña el objetivo no se consigue (TODO revisar esto).

TODO poner arquitecturas y el bloque convolucional de ResNet-18


\subsection{Datasets}

Se usarán datasets distintos para los MLP y las ConvNets, los datasets usados en MLP serán más pequeños, mientras que los usados para ConvNets serán problemas de imagen con mucha más cantidad de datos. Se decide no hacer preprocesado de los datos, ya que, aunque influye, los modelos de aprendizaje profundo asumen cierta parte del trabajo de preprocesado en las capas ocultas. De igual manera lo hacemos para que los métodos operen bajo las mismas condiciones de entrada, ya que el objetivo no es obtener los mejores resultados pobiles sino ver las mejoras obtenidas por las diferentes estrategias de optimización.  


Los datasets usados con los modelos MLP serán 3:

\begin{itemize}
    \item Wine quality: 5k instancias, 11 características, 1700 citas, clasificación y regresión
    \item Breast Cancer Winsconsin: 500 instancias, 30 características, 600 citas, clasificación
    \item Infrared Thermography Temperature: 1k instancias, 33 características, 21 citas, regresión (https://archive.ics.uci.edu/dataset/925/infrared+thermography+temperature+dataset)
\end{itemize}

El dataset de Wine Quality se usará tanto con la tarea de regresión como con la de clasificación, obteniendo así 4 tareas en total por cada modelo MLP, teniendo un total de 16 pruebas. Con esta elección de datasets se busca tener un equilibrio entre clasificación y regresión, con un par de tareas similares en cuanto a número de instancias y número de características.

Para las ConvNets usamos MNIST (Muy sencilla, 70k imagenes), Imagenette (10k imagenes) redimensionado a 32x32 para facilitar computación y para estar en igualdad de condiciones que las otras,  CIFAR-10 (60k imagenes). Son 3 datasets sobradamente conocidos y muy usados para probar modelos, de dificultad media-baja en general, pero no usamos más grandes o más difíciles por falta de potencia de cómputo.

\subsection{Optimizadores}

En cuanto a los optimizadores basados en gradiente descendente existen tres estrategias: usar el momento, usar learning rates adaptativos y combinar las dos. Cada una tiene sus virtudes y defectos (esto se argumenta en sección previa) y se elige respectivamente la opción más citada/conocida de cada una:  NAG, RMSProp y Adam. La elección de los hiperparámetros se hace usando los valores por defecto de PyTorch de los respectivos optimizadores, ya que estos valores concretos están ampliamente probados en estas implementaciones específicas con el objetivo de conseguir un buen rendimiento en modelos de manera general. Además repetimos que el objetivo de la experimentación no es obtener el mejor rendimiento con cada modelo sino obtener una comparativa entre ellos en diversas situaciones. 

El entrenamiento se realiza con la política de learning rate cíclicos de Leslie, usando un tamaño de batch grande (128 o 256) ya que según recientes experimentaciones esto proporciona una convergencia muy rápida (super-convergencia) y los tamaños de batch grande ayudan a la generalización con esta técnica. Para cada situación se entrenará el modelo durante un número de épocas máxima suficiente para que converga, usando una política de EarlyStopping con paciencia 4, para evitar el sobreentrenamiento. También se guardará el "mejor modelo" de cada entrenamiento, entendiendo éste por el estado del modelo en la época que menor error de validación obtenga. 

\subsection{Metaheurísticas}

Para comparar con las técnicas mostradas en la sección anterior, se eligen tres metaheurísticas:

Un algoritmo genético

Genetic Algorithm, Memetic algorithm population based combined with GD, SHADE-ILS algorithm (Lights and shadow of evolutionary algorithms... Dani Molina \& Paco Herrera. 




SHADE-ILS
Cojo los parámetros usados por Dani Molina en el paper: population 10. Para elegir el número de evaluaciones del algoritmo, analizar cuánto tarda en converger el modelo con GD y luego extrapolar para darle el mismo tiempo de ejecución a esta MH. Generalmente las MH requieren de más poder de cómputo para tener unos resultados comparables a los de GD. Esto es porque a prieri no son tan efectivas y porque además estamos entrenando a N individuos a la vez. En los experimentos más grandes no voy a ser capaz de alcanzar esa potencia de cómputo (en GD ya es bastante) por lo que una comparación interesante sería: dado el mismo poder de computo que el algoritmo de GD, cuánto se acerca la metaheurística a la técnica clásica. El tiempo no tiene por qué ser el mismo estrictamente, podría ser el doble o multiplicado por algún factor. Pero la idea es que en lugar de esperar a que alcance resultados parecidos y ver cuánto tiempo más ha invertido, darle el tiempo o x tiempo más y ver cuánto se acerca.

DUDAS:
Con ResNet18 y con SHADE-ILS en los datasets grandes no puedo hacer CV por falta de poder de cómputo, hacer en el resto igualmente y en estos no, o no hacerlo en ninguno?