\section{Experimentación}

Se ha desarrollado una batería de pruebas amplia y diversa que permita una correcta comparación entre el gradiente descendente y las técnicas metaheurísticas atendiendo a los objetivos señalados anteriormente. En la tabla \ref{table:exp} se ofrece un resumen esquemático de las pruebas que se van a realizar. Todas ellas se realizan con los siguientes optimizadores de gradiente descendente: Nesterov Accelerated Gradient (NAG), RMSProp y Adam. Para las metaheurísticas se ha elegido los ya conocidos en la literatura SHADE y SHADE-ILS, y añadiendo una versión híbrida de cada uno con el gradiente descendente. Todas estas elecciones se detallarán a lo largo de esta sección. Para el desarrollo del código se usa el lenguaje Python con las librerías PyTorch y FastAI, entre otras; implementado y ejecutado en Google Colab. El código puede encontrarse en: CODIGO.

Se debe destacar que se ejecutará el código en Colab con la versión CPU. El ordenador local en el que se realiza este TFG no tiene ni CPU que supere a la ofrecida por Colab ni tarjeta gráfica NVida en la que se pueda aprovechar CUDA. Además el tiempo de GPU ofrecido gratis por colab no es suficiente para la realización de la experimentación y pruebas previas que sean necesarias. Esto no afecta al objetivo, que es establecer una comparación justa entre los algoritmos de aprendizajes usados. Aunque esta opción sea más lenta, permite ofrecer una comparativa equitativa y mucho más cómoda para la ejecución y la implementación, con las pruebas que conllevan, ya que no tiene limitaciones de tiempo. Usando otra configuración se reducirían los tiempos de ejecución, sin embargo lo que nos interesa es la comparación de los tiempos entre algoritmos y no su medida absoluta.


\begin{table}[]
\begin{tabular}{|l|cccc|cll|}
\hline
\textbf{Familia} & \multicolumn{4}{c|}{MLP}                                                                                                                               & \multicolumn{3}{c|}{ConvNets}                                         \\ \hline
\textbf{Modelo}  & \multicolumn{4}{c|}{1,2,5 y 11}                                                                                                                        & \multicolumn{3}{c|}{LeNet5 y ResNet-32}                               \\ \hline
\textbf{Datasets}           & \multicolumn{1}{c|}{ITT} & \multicolumn{1}{c|}{BCW} & \multicolumn{2}{c|}{WQ}              & \multicolumn{1}{c|}{MNIST} & \multicolumn{1}{l|}{F-MNIST} & CIFAR10-G \\ \hline
\textbf{Tarea}             & \multicolumn{1}{c|}{R}                        & \multicolumn{1}{c|}{C}            & \multicolumn{1}{c|}{R} & C & \multicolumn{3}{c|}{Clasificación de imágenes}                        \\ \hline
\end{tabular}
\caption{Resumen de la experimentación. ITT: Infrared Thermografy Temperature, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas.}
\label{table:exp}
\end{table}

\subsection{Modelos}

Usaremos dos familias de modelos: MLP y ConvNets. Con los primeros usaremos datasets tabulares para clasificación y regresión, y con los segundos datasets de imágenes para la tarea de clasificación. En la siguiente sección se detallan los datasets con sus características.

Para los MLP usaremos 5 modelos, con 1,2,5 y 11 capas ocultas cada uno. El número de neuronas por capa es una potencia de 2 y con estructura piramidal incremental, es decir primero aumentando el número de neuronas por capa y luego disminuyéndolo. Estas son elecciones comunes en la literatura ya que facilitan las operaciones por su estructura (la primera) y el tratamiento de los datos (la segunda). El objetivo es conseguir una variedad experimental que permita medir los efectos de la complejidad del modelo sobre la tarea y el overfitting además de adecuarse a las condiciones del paper de referencia, con modelos desde aproximadamente mil parámetros hasta casi 1.5M, acercándose al modelo más grande presentado en dicho paper. Para la implementación se usan modelos obtenidos de la librería FastAI por simplicidad.

\begin{itemize}

	\item{1 capa oculta}: 64 neuronas, 2.238 parámetros

	\item{2 capas ocultas}: 64 y 64 neuronas respectivamente, 6.462 parámetros

	\item{5 capas ocultas}: 64, 128, 256, 128 y 64 neuronas respectivamente, 85.310 parámetros

	\item{11 capas ocultas}: 32, 64, 128, 256, 512, 1024, 512, 256, 128, 64 y 32 neuronas respectivamente, 1.403.838 parámetros

\end{itemize}

Antes de cada capa linal hay una de BatchNorm1D, ya que es la implementación por defecto de FastAI y mejora el rendimiento en el entrenamiento. Los parámetros asociados a este tipo de capa y a los de la capa de salida van incluidos en el cómputo anterior. Se incluye al final del modelo una capa de SoftMax en caso de que la tarea sea clasificación.

Para los modelos basados en convoluciones usamos el modelo LeNet5 y una ResNet con 32 capas. El objetivo es de nuevo ofrecer una variedad experimental, con el primer modelo teniendo unos 60k parámetros mientras que el segundo tiene 1.5M, imitando de nuevo las condiciones del paper de referencia. Combinamos así un modelo más básico con uno más reciente y de mayor complejidad.


LeNet5 es un modelo de sobra conocido presentado por Yann LeCun en REFERENCIA, al que sustituimos las funciones de activación sigmoide (MIRAR ESTO) por funciones de activación ReLU, ya que en la literatura posterior a la presentación del modelo se han demostrado superiores a las sigmoides. También se han sustituido las capas de AveragePool por MaxPool y añadido capas de BatchNorm por los mismos motivos. En la tabla \ref{table:lenet5} se muestra la topología de este modelo, obviando las capas de Flatten y de SoftMax. Tiene un total de AÑADIR PARÁMETROS.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\begin{tabular}{|c|cc|c|cc|}
\hline
\multirow{2}{*}{\textbf{Capa}} & \multicolumn{2}{c|}{\textbf{Imagen}}      & \multirow{2}{*}{\textbf{Kernel}} & \multicolumn{2}{c|}{\textbf{Canales}}    \\ \cline{5-6} 
                               & Entrada                    & Salida       &                                  & Entrada                & Salida \\ \hline
Convolución                    & \multicolumn{1}{c|}{32x32} & 28x28        & 5x5                              & \multicolumn{1}{c|}{1} & 6      \\ \hline
BatchNorm2D                    & \multicolumn{1}{c|}{28x28} & 28x28        & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
ReLU Activation                & \multicolumn{1}{c|}{28x28} & 28x28        & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
Max Pool                           & \multicolumn{1}{c|}{28x28} & 14x14        & 2x2, stride 2                              & \multicolumn{1}{c|}{-} & -      \\ \hline
Convolución                    & \multicolumn{1}{c|}{14x14} & 10x10        & 5x5                              & \multicolumn{1}{c|}{6} & 16     \\ \hline
BatchNorm2D                    & \multicolumn{1}{c|}{10x10} & 10x10        & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
ReLU Activation                & \multicolumn{1}{c|}{10x10} & 10x10        & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
Max Pooling                    & \multicolumn{1}{c|}{10x10} & 5x5          & 2x2                              & \multicolumn{1}{c|}{-} & -      \\ \hline
Fully Connected                & \multicolumn{1}{c|}{400}   & 120          & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
BatchNorm1D                    & \multicolumn{1}{c|}{120}   & 120          & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
ReLU Activation                & \multicolumn{1}{c|}{120}   & 120          & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
Fully Connected                & \multicolumn{1}{c|}{120}   & 84           & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
BatchNorm1D                    & \multicolumn{1}{c|}{84}    & 84           & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
ReLU Activation                & \multicolumn{1}{c|}{84}    & 84           & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
Fully Connected                & \multicolumn{1}{c|}{84}    & num\_classes & -                                & \multicolumn{1}{c|}{-} & -      \\ \hline
\end{tabular}

\caption{Topología de LeNet5 para imágenes 32x32.}
\label{table:lenet5}
\end{table}


Se ha diseñado el modelo de ResNet con 32 capas con la intención de aproximarse al número de parámetros del modelo más grande del paper de referencia y aprovechar la estructura de esta familia de modelos. Las ResNets se caracterizan por usar bloques convolucionales que agrupan varias capas de convolución donde al final de cada bloque se suma la entrada del mismo, con el objetivo de evitar el problema del desvanecimiento de gradiente (ver sección REF). En un modelo con pocas capas no se aprecia tan bien este efecto.

Los bloques convolucionales agrupan 3 capas de convolución con sus respectivas capas BatchNorm, y se usan convoluciones 1x1 para hacer cuello de botella, reduciendo así el número de parámetros sin perder expresividad de la red REFERENCIA. Se sigue el diseño usual, por ejemplo agrupando más bloques convolucionales en mitad de la red y usando solo una capa lineal. El modelo ResNet32 que se implementa tiene un total de PARAMETROS. Su topología se detalla en la tabla \ref{table:resnet32}.



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\begin{tabular}{|c|cc|c|cc|}
\hline
\multirow{2}{*}{\textbf{Capa}} & \multicolumn{2}{c|}{\textbf{Dimensión}}     & \multirow{2}{*}{\textbf{Kernel/Stride}} & \multicolumn{2}{c|}{\textbf{Canales}} \\ \cline{2-3} \cline{5-6} 
                               & \multicolumn{1}{c|}{Entrada} & Salida       &                                         & \multicolumn{1}{c|}{Entrada} & Salida \\ \hline
Convolución                    & \multicolumn{1}{c|}{32x32}   & 26x26        & 7x7                                     & \multicolumn{1}{c|}{1}       & 64     \\ \hline
BatchNorm2d                    & \multicolumn{1}{c|}{26x26}   & 26x26        & -                                       & \multicolumn{1}{c|}{-}        &   -     \\ \hline
ReLU                           & \multicolumn{1}{c|}{26x26}   & 26x26        & -                                       & \multicolumn{1}{c|}{-}        &  -      \\ \hline
MaxPool2d                      & \multicolumn{1}{c|}{26x26}   & 13x13        & 2x2, stride 2, padding 1                & \multicolumn{1}{c|}{-}        &   -     \\ \hline
BottleneckBlock x3             & \multicolumn{1}{c|}{13x13}   & 13x13        & 1x1, 3x3, 1x1                           & \multicolumn{1}{c|}{64}      & 64     \\ \hline
BottleneckBlock x4             & \multicolumn{1}{c|}{13x13}   & 7x7          & 1x1, 3x3, 1x1, stride 2                 & \multicolumn{1}{c|}{64}      & 128    \\ \hline
BottleneckBlock x4             & \multicolumn{1}{c|}{7x7}     & 4x4          & 1x1, 3x3, 1x1, stride 2                 & \multicolumn{1}{c|}{128}     & 256    \\ \hline
BottleneckBlock x3             & \multicolumn{1}{c|}{4x4}     & 2x2          & 1x1, 3x3, 1x1, stride 2                 & \multicolumn{1}{c|}{256}     & 512    \\ \hline
AdaptiveAvgPool2d              & \multicolumn{1}{c|}{2x2}     & 512          & -                                       & \multicolumn{1}{c|}{-}        & -       \\ \hline
BatchNorm1d                    & \multicolumn{1}{c|}{512}     & 512          & -                                       & \multicolumn{1}{c|}{-}        &  -      \\ \hline
Dropout                        & \multicolumn{1}{c|}{512}     & 512          & -                                       & \multicolumn{1}{c|}{-}        &     -   \\ \hline
Linear                         & \multicolumn{1}{c|}{512}     & num\_classes & -                                       & \multicolumn{1}{c|}{-}        &   -     \\ \hline
\end{tabular}
\caption{Topología de ResNet32 para imágenes 32x32.}
\label{table:resnet32}
\end{table}


\subsection{Datasets}

\subsubsection{Tabulares}

Wine quality
https://www.semanticscholar.org/paper/Modeling-wine-preferences-by-data-mining-from-Cortez-Cerdeira/bf15a0ccc14ac1deb5cea570c870389c16be019c
https://archive.ics.uci.edu/dataset/186/wine+quality

\subsubsection{Imágenes}

MNIST y FASHION-MNIST porque son los más comentados en el paper de referencia. el primero muy fácil y el segundo más complicado, por lo que ofrecen una buena visión. CIFAR-10 pq es más difícil.

\subsection{Optimizadores}

\subsection{Metaheurísticas}
