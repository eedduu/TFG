\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\abx@aux@refcontext{anyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Parte matemática: gradiente descendente y \textit  {backpropagation}}{1}{part.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{mitchell1997machine}
\abx@aux@segm{0}{0}{mitchell1997machine}
\abx@aux@cite{0}{lecun2015deep}
\abx@aux@segm{0}{0}{lecun2015deep}
\abx@aux@cite{0}{NIPS2012_c399862d}
\abx@aux@segm{0}{0}{NIPS2012_c399862d}
\abx@aux@cite{0}{Sejnowski18}
\abx@aux@segm{0}{0}{Sejnowski18}
\abx@aux@cite{0}{lecunnDeepForAI}
\abx@aux@segm{0}{0}{lecunnDeepForAI}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{2}{section.1}\protected@file@percent }
\abx@aux@cite{0}{rumelbackprop}
\abx@aux@segm{0}{0}{rumelbackprop}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{patternrecog}
\abx@aux@segm{0}{0}{patternrecog}
\abx@aux@cite{0}{EffBackProp}
\abx@aux@segm{0}{0}{EffBackProp}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{alternativabacknumerical}
\abx@aux@segm{0}{0}{alternativabacknumerical}
\abx@aux@cite{0}{alternativabackprop1}
\abx@aux@segm{0}{0}{alternativabackprop1}
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:1.GD}{{1}{3}{Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \cite {GoodFellowBook}}{figure.1}{}}
\abx@aux@cite{0}{NPHardProblem}
\abx@aux@segm{0}{0}{NPHardProblem}
\abx@aux@cite{0}{Problem3_accel}
\abx@aux@segm{0}{0}{Problem3_accel}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivación}{4}{subsection.1.1}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objetivos}{5}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentos previos}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cálculo diferencial}{5}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Lipschitz}{7}{subsection.2.2}\protected@file@percent }
\abx@aux@cite{0}{Curry1944GDNoLin}
\abx@aux@segm{0}{0}{Curry1944GDNoLin}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradiente Descendente}{8}{section.3}\protected@file@percent }
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradiente descendente de Cauchy}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradiente descendente en el entrenamiento de modelos}{9}{subsection.3.2}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\newlabel{eq:GD}{{1}{10}{Gradiente descendente en el entrenamiento de modelos}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Estrategias de gradiente descendente}{10}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec:estrategias}{{3.2.1}{10}{Estrategias de gradiente descendente}{subsubsection.3.2.1}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}\textit  {Learning rate}}{11}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualización de cómo afecta el \textit  {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \FN@sf@gobble@opt  {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}}{12}{figure.2}\protected@file@percent }
\newlabel{fig:lr}{{2}{12}{Visualización de cómo afecta el \textit {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \footnote {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Subgradientes}{12}{subsection.3.3}\protected@file@percent }
\newlabel{sec:subgrad}{{3.3}{12}{Subgradientes}{subsection.3.3}{}}
\abx@aux@cite{0}{convexSubgrad}
\abx@aux@segm{0}{0}{convexSubgrad}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Función ReLU y algunas de sus variantes más usadas como funciones de activación.\FN@sf@gobble@opt  {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}}{13}{figure.3}\protected@file@percent }
\newlabel{fig:3.ReLU}{{3}{13}{Función ReLU y algunas de sus variantes más usadas como funciones de activación.\footnote {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}{figure.3}{}}
\newlabel{prop:subgrad}{{3.1}{14}{Existencia de subgradientes}{proposicion.3.1}{}}
\newlabel{proof1:f(x)}{{2}{14}{Subgradientes}{equation.3.2}{}}
\newlabel{proof1:f(y)}{{3}{14}{Subgradientes}{equation.3.3}{}}
\newlabel{proof1:epi}{{4}{15}{Subgradientes}{equation.3.4}{}}
\abx@aux@cite{0}{ReLuat0}
\abx@aux@segm{0}{0}{ReLuat0}
\abx@aux@cite{0}{Ahmadi_2011_NP_Convex}
\abx@aux@segm{0}{0}{Ahmadi_2011_NP_Convex}
\newlabel{ej:RELUsub}{{3.1}{16}{Subgradiente de la función ReLU}{ejemplo.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convergencia}{16}{subsection.3.4}\protected@file@percent }
\newlabel{sec:convergencia}{{3.4}{16}{Convergencia}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Resultados teóricos para la convergencia del gradiente descendente}{17}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{proof:gdconvex}{{3.2}{17}{Convergencia para funciones convexas}{teorema.3.2}{}}
\newlabel{eq:gdproof1}{{5}{18}{Resultados teóricos para la convergencia del gradiente descendente}{equation.3.5}{}}
\abx@aux@cite{0}{RobbinSiegmund}
\abx@aux@segm{0}{0}{RobbinSiegmund}
\abx@aux@cite{0}{RobbinSiegmundtoSGD}
\abx@aux@segm{0}{0}{RobbinSiegmundtoSGD}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Problemas en la convergencia}{20}{subsubsection.3.4.2}\protected@file@percent }
\abx@aux@cite{0}{dauphin2014SaddlePoints}
\abx@aux@segm{0}{0}{dauphin2014SaddlePoints}
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{toc}{\contentsline {section}{\numberline {4}BP}{22}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diferenciación automática}{22}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Diferenciación hacia delante vs hacia atrás en un MLP}{23}{subsection.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Diferenciación hacia delante}}{25}{algorithm.1}\protected@file@percent }
\newlabel{alg:fowdiff}{{1}{25}{Diferenciación hacia delante vs hacia atrás en un MLP}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Diferenciación en modo reverso}}{26}{algorithm.2}\protected@file@percent }
\newlabel{alg:backdif}{{2}{26}{Diferenciación hacia delante vs hacia atrás en un MLP}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}BP para MLP}{26}{subsection.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces BP para MLP con k capas}}{27}{algorithm.3}\protected@file@percent }
\newlabel{alg:BPMLPk}{{3}{27}{BP para MLP}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Capa no-lineal}{28}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Capa Cross Entropy}{28}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Capa lineal}{29}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Grafos computacionales}{30}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representación del grafodirigido acíclico (imagen obtenida de \ref {murphy2022probabilistic})}}{31}{figure.4}\protected@file@percent }
\newlabel{fig:def.grafo}{{4}{31}{Representación del grafodirigido acíclico (imagen obtenida de \ref {murphy2022probabilistic})}{figure.4}{}}
\abx@aux@cite{0}{VanishExplode}
\abx@aux@segm{0}{0}{VanishExplode}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Problemas con el cálculo del gradiente}{32}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Desvanecimiento y explosión del gradiente}{32}{subsubsection.4.4.1}\protected@file@percent }
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Inicialización de los pesos}{33}{subsubsection.4.4.2}\protected@file@percent }
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\abx@aux@cite{0}{heinic}
\abx@aux@segm{0}{0}{heinic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Parte informática: enfoque clásico vs técnicas metaheurísticas}{35}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Introducción}{36}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivación}{37}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Objetivos}{38}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Bibliografía}{38}{Item.9}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{nohash}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{Ahmadi_2011_NP_Convex}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{AutomaticDiff}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ReLuat0}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patternrecog}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecunnDeepForAI}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{convexSubgrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{CauchyGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Curry1944GDNoLin}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dauphin2014SaddlePoints}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Problem3_accel}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{RobbinSiegmund}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stabilityProblem2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{GoodFellowBook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{RobbinSiegmundtoSGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heinic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{VanishExplode}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NIPS2012_c399862d}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun2015deep}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{EffBackProp}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machine}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NPHardProblem}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{murphy2022probabilistic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabackprop1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabacknumerical}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelbackprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Sejnowski18}{anyt/global//global/global/global}
\gdef \@abspage@last{43}
