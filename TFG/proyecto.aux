\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\abx@aux@refcontext{anyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Parte matemática: análisis del gradiente descendente, su convergencia y \textit  {backpropagation}.}{1}{part.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{mitchell1997machine}
\abx@aux@segm{0}{0}{mitchell1997machine}
\abx@aux@cite{0}{lecun2015deep}
\abx@aux@segm{0}{0}{lecun2015deep}
\abx@aux@cite{0}{NIPS2012_c399862d}
\abx@aux@segm{0}{0}{NIPS2012_c399862d}
\abx@aux@cite{0}{Sejnowski18}
\abx@aux@segm{0}{0}{Sejnowski18}
\abx@aux@cite{0}{lecunnDeepForAI}
\abx@aux@segm{0}{0}{lecunnDeepForAI}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{2}{section.1}\protected@file@percent }
\abx@aux@cite{0}{rumelbackprop}
\abx@aux@segm{0}{0}{rumelbackprop}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{patternrecog}
\abx@aux@segm{0}{0}{patternrecog}
\abx@aux@cite{0}{EffBackProp}
\abx@aux@segm{0}{0}{EffBackProp}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{alternativabacknumerical}
\abx@aux@segm{0}{0}{alternativabacknumerical}
\abx@aux@cite{0}{alternativabackprop1}
\abx@aux@segm{0}{0}{alternativabackprop1}
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.GD}{{1}{3}{Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \cite {GoodFellowBook}}{figure.caption.2}{}}
\abx@aux@cite{0}{NPHardProblem}
\abx@aux@segm{0}{0}{NPHardProblem}
\abx@aux@cite{0}{Problem3_accel}
\abx@aux@segm{0}{0}{Problem3_accel}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivación}{4}{subsection.1.1}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objetivos}{5}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentos previos}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cálculo diferencial}{5}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Algunos conceptos sobre probabilidad}{9}{subsection.2.2}\protected@file@percent }
\newlabel{def:esprob}{{2.9}{9}{Espacio de probabilidad}{definicion.2.9}{}}
\abx@aux@cite{0}{Curry1944GDNoLin}
\abx@aux@segm{0}{0}{Curry1944GDNoLin}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradiente Descendente}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradiente descendente de Cauchy}{13}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradiente descendente en el entrenamiento de modelos}{13}{subsection.3.2}\protected@file@percent }
\newlabel{eq:GD}{{2}{13}{Gradiente descendente en el entrenamiento de modelos}{equation.3.2}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Estrategias de gradiente descendente}{14}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec:estrategias}{{3.2.1}{14}{Estrategias de gradiente descendente}{subsubsection.3.2.1}{}}
\newlabel{eq:SGD}{{3}{14}{Estrategias de gradiente descendente}{equation.3.3}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualización de cómo afecta el \textit  {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \FN@sf@gobble@opt  {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}}{15}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lr}{{2}{15}{Visualización de cómo afecta el \textit {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \footnote {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}\textit  {Learning rate}}{15}{subsubsection.3.2.2}\protected@file@percent }
\abx@aux@cite{0}{convexSubgrad}
\abx@aux@segm{0}{0}{convexSubgrad}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Subgradientes}{16}{subsection.3.3}\protected@file@percent }
\newlabel{sec:subgrad}{{3.3}{16}{Subgradientes}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Función ReLU y algunas de sus variantes más usadas como funciones de activación.\FN@sf@gobble@opt  {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:3.ReLU}{{3}{17}{Función ReLU y algunas de sus variantes más usadas como funciones de activación.\footnote {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}{figure.caption.4}{}}
\newlabel{prop:subgrad}{{3.1}{19}{Existencia de subgradientes}{proposicion.3.1}{}}
\newlabel{eq:des2}{{4}{20}{Subgradientes}{equation.3.4}{}}
\newlabel{proof1:f(x)}{{5}{20}{Subgradientes}{equation.3.5}{}}
\newlabel{proof1:f(y)}{{6}{20}{Subgradientes}{equation.3.6}{}}
\abx@aux@cite{0}{ReLuat0}
\abx@aux@segm{0}{0}{ReLuat0}
\newlabel{proof1:epi}{{7}{21}{Subgradientes}{equation.3.7}{}}
\newlabel{ej:RELUsub}{{3.1}{21}{Subgradiente de la función ReLU}{ejemplo.3.1}{}}
\abx@aux@cite{0}{Ahmadi_2011_NP_Convex}
\abx@aux@segm{0}{0}{Ahmadi_2011_NP_Convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convergencia}{22}{subsection.3.4}\protected@file@percent }
\newlabel{sec:convergencia}{{3.4}{22}{Convergencia}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Convergencia para \textit  {Batch Gradient Descent}}{22}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{proof:gdconvex}{{3.2}{23}{Convergencia para funciones convexas}{teorema.3.2}{}}
\newlabel{eq:gdproof1}{{8}{24}{Convergencia para \textit {Batch Gradient Descent}}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Convergencia para versiones estocásticas}{25}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{eq:martingala}{{9}{26}{Convergencia para versiones estocásticas}{equation.3.9}{}}
\newlabel{teor:sig}{{3.3}{27}{Teorema de Robbins-Siegmund}{teorema.3.3}{}}
\newlabel{eq:sig1}{{11}{28}{Convergencia para versiones estocásticas}{equation.3.11}{}}
\newlabel{teor:convsgd}{{3.4}{28}{Convergencia de algoritmos SGD}{teorema.3.4}{}}
\newlabel{eq:convsgd1}{{12}{29}{Convergencia de algoritmos SGD}{equation.3.12}{}}
\newlabel{eq:convsgd2}{{13}{29}{Convergencia de algoritmos SGD}{equation.3.13}{}}
\abx@aux@cite{0}{dauphin2014SaddlePoints}
\abx@aux@segm{0}{0}{dauphin2014SaddlePoints}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Problemas en la convergencia}{30}{subsubsection.3.4.3}\protected@file@percent }
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{toc}{\contentsline {section}{\numberline {4}\textit  {Backpropagation}}{31}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diferenciación automática}{31}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Diferenciación hacia delante vs hacia atrás en un MLP}{32}{subsection.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Diferenciación hacia delante}}{34}{algorithm.1}\protected@file@percent }
\newlabel{alg:fowdiff}{{1}{34}{Diferenciación hacia delante}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Diferenciación en modo reverso}}{35}{algorithm.2}\protected@file@percent }
\newlabel{alg:backdif}{{2}{35}{Diferenciación en modo reverso}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\textit  {Backpropagation} en perceptrones multicapa}{35}{subsection.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces BP para MLP con k capas}}{37}{algorithm.3}\protected@file@percent }
\newlabel{alg:BPMLPk}{{3}{37}{BP para MLP con k capas}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Capa no-lineal}{37}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Capa Cross Entropy}{38}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Capa lineal}{38}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Grafos computacionales}{39}{subsubsection.4.3.4}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representación del grafo dirigido acíclico (imagen obtenida de \blx@tocontentsinit {0}\cite {murphy2022probabilistic})}}{40}{figure.caption.5}\protected@file@percent }
\newlabel{fig:def.grafo}{{4}{40}{Representación del grafo dirigido acíclico (imagen obtenida de \cite {murphy2022probabilistic})}{figure.caption.5}{}}
\abx@aux@cite{0}{VanishExplode}
\abx@aux@segm{0}{0}{VanishExplode}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Problemas con el cálculo del gradiente}{41}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Desvanecimiento y explosión del gradiente}{41}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:desvyexpl}{{4.4.1}{41}{Desvanecimiento y explosión del gradiente}{subsubsection.4.4.1}{}}
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Inicialización de los pesos}{42}{subsubsection.4.4.2}\protected@file@percent }
\newlabel{sec:inipesos}{{4.4.2}{42}{Inicialización de los pesos}{subsubsection.4.4.2}{}}
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\abx@aux@cite{0}{heinic}
\abx@aux@segm{0}{0}{heinic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusiones y trabajos futuros}{43}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Trabajos futuros}{45}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Parte informática: Estudio empírico comparativo entre gradiente descendiente y metaheurísticas para el entrenamiento de redes neuronales.}{46}{part.2}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{ResNets}
\abx@aux@segm{0}{0}{ResNets}
\abx@aux@cite{0}{MHDef}
\abx@aux@segm{0}{0}{MHDef}
\abx@aux@cite{0}{MHoverview}
\abx@aux@segm{0}{0}{MHoverview}
\@writefile{toc}{\contentsline {section}{\numberline {6}Introducción}{47}{section.6}\protected@file@percent }
\abx@aux@cite{0}{MHDef}
\abx@aux@segm{0}{0}{MHDef}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Motivación}{48}{subsection.6.1}\protected@file@percent }
\newlabel{sec:motinfo}{{6.1}{48}{Motivación}{subsection.6.1}{}}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Objetivos}{49}{subsection.6.2}\protected@file@percent }
\newlabel{sec:objinf}{{6.2}{49}{Objetivos}{subsection.6.2}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{mhhandbook}
\abx@aux@segm{0}{0}{mhhandbook}
\abx@aux@cite{0}{diffevbook}
\abx@aux@segm{0}{0}{diffevbook}
\abx@aux@cite{0}{Numerical_optimization}
\abx@aux@segm{0}{0}{Numerical_optimization}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{perceptron}
\abx@aux@segm{0}{0}{perceptron}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\@writefile{toc}{\contentsline {section}{\numberline {7}Fundamentos teóricos}{51}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Redes neuronales y aprendizaje profundo}{51}{subsection.7.1}\protected@file@percent }
\newlabel{sec:profundo}{{7.1}{51}{Redes neuronales y aprendizaje profundo}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Red neuronal}{51}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Esquema del modelo de una neurona con tres conexiones de entrada. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{52}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Perceptron}{{5}{52}{Esquema del modelo de una neurona con tres conexiones de entrada. Obtenida de \cite {stanford_231}}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Aprendizaje profundo y redes neuronales profundas}{52}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Perceptrones multicapa}{52}{subsubsection.7.1.3}\protected@file@percent }
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{lenet5}
\abx@aux@segm{0}{0}{lenet5}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Red neuronal de tres capas (sin contar la capa de entrada) con dos capas ocultas de cuatro neuronas cada una y una capa de salida con una neurona. Destacar que cada neurona se conecta solo con la siguiente capa. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{53}{figure.caption.7}\protected@file@percent }
\newlabel{fig:NeuralNet}{{6}{53}{Red neuronal de tres capas (sin contar la capa de entrada) con dos capas ocultas de cuatro neuronas cada una y una capa de salida con una neurona. Destacar que cada neurona se conecta solo con la siguiente capa. Obtenida de \cite {stanford_231}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}ConvNets}{53}{subsection.7.2}\protected@file@percent }
\newlabel{sec:convnets}{{7.2}{53}{ConvNets}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Operación de convolución}{53}{subsubsection.7.2.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Capa Convolucional}{54}{subsubsection.7.2.2}\protected@file@percent }
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convolución 2D sin voltear el filtro (relación cruzada). Obtenida de \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{55}{figure.caption.8}\protected@file@percent }
\newlabel{fig:3.Conv}{{7}{55}{Convolución 2D sin voltear el filtro (relación cruzada). Obtenida de \cite {GoodFellowBook}}{figure.caption.8}{}}
\newlabel{eq:output}{{14}{55}{Capa Convolucional}{equation.7.14}{}}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{batchnorm}
\abx@aux@segm{0}{0}{batchnorm}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ilustración de la disposición espacial. El tamaño de la entrada es $W=5$ (vector gris) y el del filtro $F=3$ (vector verde), sin usar \textit  {padding} ($P=1$). En el ejemplo de la izquierda se usa \textit  {stride} $S=1$, mientras que en el de la derecha se usa $S=2$, obteniendo tamaños de salida de 5 y 3, respectivamente. Estos tamaños se pueden calcular según la fórmula \ref {eq:output}. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{56}{figure.caption.9}\protected@file@percent }
\newlabel{fig:stride}{{8}{56}{Ilustración de la disposición espacial. El tamaño de la entrada es $W=5$ (vector gris) y el del filtro $F=3$ (vector verde), sin usar \textit {padding} ($P=1$). En el ejemplo de la izquierda se usa \textit {stride} $S=1$, mientras que en el de la derecha se usa $S=2$, obteniendo tamaños de salida de 5 y 3, respectivamente. Estos tamaños se pueden calcular según la fórmula \ref {eq:output}. Obtenida de \cite {stanford_231}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Capa \textit  {Pooling}}{56}{subsubsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}Capa \textit  {Batch Normalization}}{56}{subsubsection.7.2.4}\protected@file@percent }
\abx@aux@cite{0}{ResNets}
\abx@aux@segm{0}{0}{ResNets}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Capa de \textit  {pooling} con un filtro de tamaño $2 \times 2$ y \textit  {stride} 2. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{57}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pooling}{{9}{57}{Capa de \textit {pooling} con un filtro de tamaño $2 \times 2$ y \textit {stride} 2. Obtenida de \cite {stanford_231}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.5}Capa totalmente conectada}{57}{subsubsection.7.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}ResNets}{57}{subsection.7.3}\protected@file@percent }
\newlabel{sec:resnets}{{7.3}{57}{ResNets}{subsection.7.3}{}}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{bottleorig}
\abx@aux@segm{0}{0}{bottleorig}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces En un bloque convolucional estándar (izquierda), la parte dentro de la línea de puntos debe aprender directamente la función $f(x)$. En un bloque residual (derecha), la parte dentro de la línea de puntos debe aprender la función residual $g(x)=f(x)-x$, haciendo que la función identidad $f(x)=x$ sea más fácil de aprender. Obtenida de \blx@tocontentsinit {0}\cite {divedeeplearning}}}{58}{figure.caption.11}\protected@file@percent }
\newlabel{fig:resblock}{{10}{58}{En un bloque convolucional estándar (izquierda), la parte dentro de la línea de puntos debe aprender directamente la función $f(x)$. En un bloque residual (derecha), la parte dentro de la línea de puntos debe aprender la función residual $g(x)=f(x)-x$, haciendo que la función identidad $f(x)=x$ sea más fácil de aprender. Obtenida de \cite {divedeeplearning}}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Bloques residuales}{58}{subsubsection.7.3.1}\protected@file@percent }
\abx@aux@cite{0}{leslie1}
\abx@aux@segm{0}{0}{leslie1}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Bloque residual donde la entrada tiene la misma dimensión que la salida (izquierda), y bloque residual donde se transforma el tamaño de la entrada a través de convoluciones $1\times 1$ para que tengan el mismo tamaño (derecha). Obtenida de \blx@tocontentsinit {0}\cite {divedeeplearning}}}{59}{figure.caption.12}\protected@file@percent }
\newlabel{fig:resblock1x1}{{11}{59}{Bloque residual donde la entrada tiene la misma dimensión que la salida (izquierda), y bloque residual donde se transforma el tamaño de la entrada a través de convoluciones $1\times 1$ para que tengan el mismo tamaño (derecha). Obtenida de \cite {divedeeplearning}}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Convoluciones 1x1}{59}{subsubsection.7.3.2}\protected@file@percent }
\abx@aux@cite{0}{leslie2}
\abx@aux@segm{0}{0}{leslie2}
\abx@aux@cite{0}{leslie3}
\abx@aux@segm{0}{0}{leslie3}
\abx@aux@cite{0}{momentumorig}
\abx@aux@segm{0}{0}{momentumorig}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Política de un ciclo de Leslie}{60}{subsection.7.4}\protected@file@percent }
\newlabel{sec:leslie}{{7.4}{60}{Política de un ciclo de Leslie}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Optimizadores de gradiente descendente}{60}{subsection.7.5}\protected@file@percent }
\newlabel{sec:gd}{{7.5}{60}{Optimizadores de gradiente descendente}{subsection.7.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}NAG}{60}{subsubsection.7.5.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{Nesterov}
\abx@aux@segm{0}{0}{Nesterov}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparación entre el algoritmo de gradiente descendente sin usar el método del momento (izquierda) y usándolo (derecha). Vemos que en la figura de la derecha se diferencia la dirección del gradiente (flechas negras) y el camino seguido por el algoritmo en rojo. Imágenes obtenidas de \blx@tocontentsinit {0}\cite {GoodFellowBook}.}}{61}{figure.caption.13}\protected@file@percent }
\newlabel{fig:momentum}{{12}{61}{Comparación entre el algoritmo de gradiente descendente sin usar el método del momento (izquierda) y usándolo (derecha). Vemos que en la figura de la derecha se diferencia la dirección del gradiente (flechas negras) y el camino seguido por el algoritmo en rojo. Imágenes obtenidas de \cite {GoodFellowBook}}{figure.caption.13}{}}
\abx@aux@cite{0}{rmsprop}
\abx@aux@segm{0}{0}{rmsprop}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparación entre los métodos del momento original (vectores azules) y el momento de Nesterov. En este último, primero realizamos un salto grande en la dirección del gradiente acumulado (vector marrrón) para luego medir el gradiente de la posición al acabar el salto y realizar una corrección (vector rojo). La flecha verde indica la posición final corregida donde acaba una iteración del método NAG. Obtenida de \url  {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}}{62}{figure.caption.14}\protected@file@percent }
\newlabel{fig:resblock1x1}{{13}{62}{Comparación entre los métodos del momento original (vectores azules) y el momento de Nesterov. En este último, primero realizamos un salto grande en la dirección del gradiente acumulado (vector marrrón) para luego medir el gradiente de la posición al acabar el salto y realizar una corrección (vector rojo). La flecha verde indica la posición final corregida donde acaba una iteración del método NAG. Obtenida de \url {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}RMSProp}{62}{subsubsection.7.5.2}\protected@file@percent }
\abx@aux@cite{0}{adagrad}
\abx@aux@segm{0}{0}{adagrad}
\abx@aux@cite{0}{Adam}
\abx@aux@segm{0}{0}{Adam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Adam}{63}{subsubsection.7.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Metaheurísticas}{63}{subsection.7.6}\protected@file@percent }
\newlabel{sec:mh}{{7.6}{63}{Metaheurísticas}{subsection.7.6}{}}
\abx@aux@cite{0}{holland_gen_alg}
\abx@aux@segm{0}{0}{holland_gen_alg}
\abx@aux@cite{0}{holland_gen_alg}
\abx@aux@segm{0}{0}{holland_gen_alg}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Famosa imagen esquemática del operador de cruce en un punto para dos vectores binarios de soluciones (izquierda), comparando el proceso con la recombinación genética de cromosomas (derecha). Obtenida de \blx@tocontentsinit {0}\cite {holland_gen_alg}}}{64}{figure.caption.15}\protected@file@percent }
\newlabel{fig:cruce_mh}{{14}{64}{Famosa imagen esquemática del operador de cruce en un punto para dos vectores binarios de soluciones (izquierda), comparando el proceso con la recombinación genética de cromosomas (derecha). Obtenida de \cite {holland_gen_alg}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.1}Metaheurísticas basadas en poblaciones}{64}{subsubsection.7.6.1}\protected@file@percent }
\abx@aux@cite{0}{diffev}
\abx@aux@segm{0}{0}{diffev}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Esquema de la ejecución de un algoritmo basado en poblaciones donde se observan las etapas de cada generación. Obtenida de \url  {https://blogs.imf-formacion.com/blog/tecnologia/}}}{65}{figure.caption.16}\protected@file@percent }
\newlabel{fig:gen_alg}{{15}{65}{Esquema de la ejecución de un algoritmo basado en poblaciones donde se observan las etapas de cada generación. Obtenida de \url {https://blogs.imf-formacion.com/blog/tecnologia/}}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.2}Differential Evolution}{65}{subsubsection.7.6.2}\protected@file@percent }
\abx@aux@cite{0}{L-BFGS-B}
\abx@aux@segm{0}{0}{L-BFGS-B}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Esquema general de DE}}{66}{algorithm.4}\protected@file@percent }
\newlabel{alg:de}{{4}{66}{Esquema general de DE}{algorithm.4}{}}
\abx@aux@cite{0}{BFGS}
\abx@aux@segm{0}{0}{BFGS}
\abx@aux@cite{0}{L-BFGS}
\abx@aux@segm{0}{0}{L-BFGS}
\abx@aux@cite{0}{shade}
\abx@aux@segm{0}{0}{shade}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.3}L-BFGS-B}{67}{subsubsection.7.6.3}\protected@file@percent }
\newlabel{sec:l-bfgs}{{7.6.3}{67}{L-BFGS-B}{subsubsection.7.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.4}SHADE}{67}{subsubsection.7.6.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Algoritmo SHADE}}{68}{algorithm.5}\protected@file@percent }
\newlabel{alg:shade}{{5}{68}{Algoritmo SHADE}{algorithm.5}{}}
\abx@aux@cite{0}{nofreelunch}
\abx@aux@segm{0}{0}{nofreelunch}
\abx@aux@cite{0}{shadeils}
\abx@aux@segm{0}{0}{shadeils}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.5}Algoritmos meméticos}{69}{subsubsection.7.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.6.6}SHADE-ILS}{69}{subsubsection.7.6.6}\protected@file@percent }
\newlabel{sec:shade-ils}{{7.6.6}{69}{SHADE-ILS}{subsubsection.7.6.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Algoritmo SHADE-ILS}}{70}{algorithm.6}\protected@file@percent }
\newlabel{alg:shade-ils}{{6}{70}{Algoritmo SHADE-ILS}{algorithm.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Números de artículos publicados por año en la web SCOPUS para la búsqueda TITLE-ABS-KEY ( deep AND learning AND training ), muestra el número de artículos por año.}}{71}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scopus_deep}{{16}{71}{Números de artículos publicados por año en la web SCOPUS para la búsqueda TITLE-ABS-KEY ( deep AND learning AND training ), muestra el número de artículos por año}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Estado del arte}{71}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Número de arrtículos publicados por año para el entrenamiento de modelos de aprendizaje profundo con metaheurísticas (arriba) y gradiente descendente (debajo) según las búsquedas correspondientes.}}{72}{figure.caption.18}\protected@file@percent }
\newlabel{fig:resEdA}{{17}{72}{Número de arrtículos publicados por año para el entrenamiento de modelos de aprendizaje profundo con metaheurísticas (arriba) y gradiente descendente (debajo) según las búsquedas correspondientes}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Gradiente descendente y optimizadores}{73}{subsection.8.1}\protected@file@percent }
\abx@aux@cite{0}{yellowfin}
\abx@aux@segm{0}{0}{yellowfin}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Tabla con los datasets utilizados con sus respectivos modelos en la experimentación de la comparativa (enlace)}}{74}{table.caption.19}\protected@file@percent }
\newlabel{table:exp}{{1}{74}{Tabla con los datasets utilizados con sus respectivos modelos en la experimentación de la comparativa (enlace)}{table.caption.19}{}}
\abx@aux@cite{0}{162}
\abx@aux@segm{0}{0}{162}
\abx@aux@cite{0}{beesalgo}
\abx@aux@segm{0}{0}{beesalgo}
\abx@aux@cite{0}{155}
\abx@aux@segm{0}{0}{155}
\abx@aux@cite{0}{163}
\abx@aux@segm{0}{0}{163}
\abx@aux@cite{0}{pso}
\abx@aux@segm{0}{0}{pso}
\abx@aux@cite{0}{174}
\abx@aux@segm{0}{0}{174}
\abx@aux@cite{0}{176}
\abx@aux@segm{0}{0}{176}
\abx@aux@cite{0}{siman}
\abx@aux@segm{0}{0}{siman}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Metaheurísticas en el entrenamiento de modelos}{75}{subsection.8.2}\protected@file@percent }
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}SHADE-ILS}{76}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas.}}{77}{table.caption.20}\protected@file@percent }
\newlabel{table:exp}{{2}{77}{Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas}{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Experimentación y entorno de ejecución}{77}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Reproducibilidad}{77}{subsection.9.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Detalles de los modelos MLP}}{78}{table.caption.21}\protected@file@percent }
\newlabel{tab:MLPmod}{{3}{78}{Detalles de los modelos MLP}{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Modelos}{78}{subsection.9.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{79}{table.caption.22}\protected@file@percent }
\newlabel{table:lenet5}{{4}{79}{Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{80}{table.caption.23}\protected@file@percent }
\newlabel{table:resnet57}{{5}{80}{Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{80}{table.caption.24}\protected@file@percent }
\newlabel{table:resnet15}{{6}{80}{Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.24}{}}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Información sobre los conjuntos de datos tabulares. El ratio de balance se calcula dividiendo el número de instancias de la clase más usual entre la menos usual. BCW: Breast Cancer Winsconsin, BHP: Boston Housing Price, WQ: Wine Quality. C: clasificación, R: regresión.}}{81}{table.caption.25}\protected@file@percent }
\newlabel{tab:dat_tab}{{7}{81}{Información sobre los conjuntos de datos tabulares. El ratio de balance se calcula dividiendo el número de instancias de la clase más usual entre la menos usual. BCW: Breast Cancer Winsconsin, BHP: Boston Housing Price, WQ: Wine Quality. C: clasificación, R: regresión}{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Conjuntos de datos}{81}{subsection.9.3}\protected@file@percent }
\newlabel{sec:conjuntos_de_datos}{{9.3}{81}{Conjuntos de datos}{subsection.9.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Tabulares}{81}{subsubsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Imágenes}{82}{subsubsection.9.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de MNIST, con sus respectivas etiquetas.}}{83}{figure.caption.26}\protected@file@percent }
\newlabel{fig:mnist}{{18}{83}{Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de MNIST, con sus respectivas etiquetas}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de F-MNIST, con sus respectivas etiquetas.}}{84}{figure.caption.27}\protected@file@percent }
\newlabel{fig:fmnist}{{19}{84}{Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de F-MNIST, con sus respectivas etiquetas}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de CIFAR-10-G, con sus respectivas etiquetas.}}{84}{figure.caption.28}\protected@file@percent }
\newlabel{fig:cifar10}{{20}{84}{Ejemplo de 10 imágenes aleatorias del conjunto de entrenamiento de CIFAR-10-G, con sus respectivas etiquetas}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Entrenamiento}{84}{subsection.9.4}\protected@file@percent }
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces \relax }}{86}{table.caption.29}\protected@file@percent }
\newlabel{tab:params}{{8}{86}{\relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Gradiente descendente}{86}{subsubsection.9.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.2}Metaheurísticas}{86}{subsubsection.9.4.2}\protected@file@percent }
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Implementación}{88}{subsection.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}Funciones auxiliares}{88}{subsubsection.9.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Metaheurísticas}{89}{subsubsection.9.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Resultados}{89}{section.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Resultados del entrenamiento sobre el conjunto de datos BCW, entrenando un modelo MLP de 1 capa a través de técnicas metaheurísticas (izquierda) y optimizadores basados en gradiente descendente (derecha).}}{90}{figure.caption.30}\protected@file@percent }
\newlabel{fig:resgen1}{{21}{90}{Resultados del entrenamiento sobre el conjunto de datos BCW, entrenando un modelo MLP de 1 capa a través de técnicas metaheurísticas (izquierda) y optimizadores basados en gradiente descendente (derecha)}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Comentarios generales}{90}{subsection.10.1}\protected@file@percent }
\newlabel{fig:resgen2}{{\caption@xref {fig:resgen2}{ on input line 22}}{91}{Comentarios generales}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Resultados del entrenamiento sobre el conjunto de datos F-MNIST, entrenando el modelo LeNet5 a través de técnicas metaheurísticas (izquierda) y optimizadores basados en gradiente descendente (derecha).}}{91}{figure.caption.31}\protected@file@percent }
\newlabel{fig:resgen3}{{\caption@xref {fig:resgen3}{ on input line 32}}{91}{Comentarios generales}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Resultados del entrenamiento sobre el conjunto de datos F-MNIST, entrenando el modelo ResNet57 a través de técnicas metaheurísticas (izquierda) y optimizadores basados en gradiente descendente (derecha).}}{91}{figure.caption.32}\protected@file@percent }
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Cuestión P1}{92}{subsection.10.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Tareas a comparar para responder a la cuestión P1.}}{93}{table.caption.33}\protected@file@percent }
\newlabel{tab:expP1}{{9}{93}{Tareas a comparar para responder a la cuestión P1}{table.caption.33}{}}
\abx@aux@cite{0}{randomforest}
\abx@aux@segm{0}{0}{randomforest}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Rendimiento medio reescalado de las técnicas metaheurísticas, medido en \textit  {accuracy}, para los conjuntos de datos tabulares según el número de capas.}}{95}{table.caption.34}\protected@file@percent }
\newlabel{tab:p1tab}{{10}{95}{Rendimiento medio reescalado de las técnicas metaheurísticas, medido en \textit {accuracy}, para los conjuntos de datos tabulares según el número de capas}{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Rendimiento medio reescalado de las técnicas metaheurísticas, medido en \textit  {accuracy}, para los conjuntos de datos de imágenes según el modelo.}}{95}{table.caption.35}\protected@file@percent }
\newlabel{tab:p1im}{{11}{95}{Rendimiento medio reescalado de las técnicas metaheurísticas, medido en \textit {accuracy}, para los conjuntos de datos de imágenes según el modelo}{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Representación del rendimiento medio reescalado de las técnicas metaheurísticas según la complejidad de la tarea, el tamaño del conjunto de datos y la complejidad del modelo.}}{96}{figure.caption.36}\protected@file@percent }
\newlabel{fig:3dP1}{{24}{96}{Representación del rendimiento medio reescalado de las técnicas metaheurísticas según la complejidad de la tarea, el tamaño del conjunto de datos y la complejidad del modelo}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Mapa de calor del rendimiento de las técnicas metaheurísticas analizando dos a dos tres factores: complejidad de la tarea, tamaño del conjunto de datos y complejidad del modelo.}}{97}{figure.caption.37}\protected@file@percent }
\newlabel{fig:hmP1}{{25}{97}{Mapa de calor del rendimiento de las técnicas metaheurísticas analizando dos a dos tres factores: complejidad de la tarea, tamaño del conjunto de datos y complejidad del modelo}{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Análisis de dependencias parciales en base al rendimiento de los modelos entrenados con técnicas metaheurísticas según la complejidad de la tarea, el tamaño del conjunto de datos y la complejidad del modelo.}}{98}{figure.caption.38}\protected@file@percent }
\newlabel{fig:P1rf}{{26}{98}{Análisis de dependencias parciales en base al rendimiento de los modelos entrenados con técnicas metaheurísticas según la complejidad de la tarea, el tamaño del conjunto de datos y la complejidad del modelo}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Cuestión P2}{98}{subsection.10.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Tiempo del entrenamiento en segundos para el conjunto de datos Breast Cancer Winsconsin.}}{98}{table.caption.39}\protected@file@percent }
\newlabel{tab:bcw_time}{{12}{98}{Tiempo del entrenamiento en segundos para el conjunto de datos Breast Cancer Winsconsin}{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Tiempo del entrenamiento en segundos para el dataset Wine Quality (Clasificación).}}{99}{table.caption.40}\protected@file@percent }
\newlabel{tab:wqc_time}{{13}{99}{Tiempo del entrenamiento en segundos para el dataset Wine Quality (Clasificación)}{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Tiempo del entrenamiento en segundos para el conjunto de datos F-MNIST.}}{99}{table.caption.41}\protected@file@percent }
\newlabel{tab:fmnist_time}{{14}{99}{Tiempo del entrenamiento en segundos para el conjunto de datos F-MNIST}{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Mayores y menores diferencias de tiempo, expresada en veces mayor, de los entrenamientos en los conjuntos de datos de BCW, WQC y F-MNIST, entre las técnicas metaheurísticas y optimizadores basados en gradiente descendente.}}{100}{table.caption.42}\protected@file@percent }
\newlabel{tab:timesmaxmin}{{15}{100}{Mayores y menores diferencias de tiempo, expresada en veces mayor, de los entrenamientos en los conjuntos de datos de BCW, WQC y F-MNIST, entre las técnicas metaheurísticas y optimizadores basados en gradiente descendente}{table.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Comparación de los tiempos de ejecución para una época de Adam, la función de error implementada sobre el conjunto de entrenamiento (Función1), y la función de error implementada para el conjunto de entrenamiento, de validación y el cálculo de \textit  {accuracy} (Función2), para los diferentes modelos indicados y en F-MNIST.}}{100}{table.caption.43}\protected@file@percent }
\newlabel{tab:base_time}{{16}{100}{Comparación de los tiempos de ejecución para una época de Adam, la función de error implementada sobre el conjunto de entrenamiento (Función1), y la función de error implementada para el conjunto de entrenamiento, de validación y el cálculo de \textit {accuracy} (Función2), para los diferentes modelos indicados y en F-MNIST}{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Tareas a comparar para la cuestión P3.}}{103}{table.caption.44}\protected@file@percent }
\newlabel{tab:expP3}{{17}{103}{Tareas a comparar para la cuestión P3}{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Entrenamiento de las técnicas metaheurísticas en el conjunto de datos BCW. E: Entrenamiento, T: Test, A: \textit  {Accuracy}.}}{103}{table.caption.45}\protected@file@percent }
\newlabel{tab:P3bcw}{{18}{103}{Entrenamiento de las técnicas metaheurísticas en el conjunto de datos BCW. E: Entrenamiento, T: Test, A: \textit {Accuracy}}{table.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Cuestión P3}{103}{subsection.10.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Entrenamiento de las técnicas metaheurísticas en el conjunto de datos WQC. E: Entrenamiento, T: Test, A: \textit  {Accuracy}.}}{104}{table.caption.46}\protected@file@percent }
\newlabel{tab:P3wqc}{{19}{104}{Entrenamiento de las técnicas metaheurísticas en el conjunto de datos WQC. E: Entrenamiento, T: Test, A: \textit {Accuracy}}{table.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Mapa de calor del rendimiento de las técnicas metaheurísticas para los datasets correspondientes, realizando una media de los valores entre las distintas capas.}}{105}{figure.caption.47}\protected@file@percent }
\newlabel{fig:P3hmdata}{{27}{105}{Mapa de calor del rendimiento de las técnicas metaheurísticas para los datasets correspondientes, realizando una media de los valores entre las distintas capas}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Mapa de calor del rendimiento de las técnicas metaheurísticas para los distintos números de capas de los modelos MLP, realizando una media de los valores entre los conjuntos de datos correspondientes.}}{106}{figure.caption.48}\protected@file@percent }
\newlabel{fig:P3hmlayer}{{28}{106}{Mapa de calor del rendimiento de las técnicas metaheurísticas para los distintos números de capas de los modelos MLP, realizando una media de los valores entre los conjuntos de datos correspondientes}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Cuestión P4}{107}{subsection.10.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces Valores del test de Wilcoxon para las hipótesis propuestas.}}{108}{table.caption.49}\protected@file@percent }
\newlabel{tab:test}{{20}{108}{Valores del test de Wilcoxon para las hipótesis propuestas}{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {21}{\ignorespaces Mayores y menores diferencias de tiempo, expresada en veces mayor, de los entrenamientos en los conjuntos de datos de BCW, WQC y F-MNIST, entre las técnicas metaheurísticas y optimizadores basados en gradiente descendente.}}{108}{table.caption.50}\protected@file@percent }
\newlabel{tab:timesmaxmin}{{21}{108}{Mayores y menores diferencias de tiempo, expresada en veces mayor, de los entrenamientos en los conjuntos de datos de BCW, WQC y F-MNIST, entre las técnicas metaheurísticas y optimizadores basados en gradiente descendente}{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22}{\ignorespaces Comparación de los resultados del entrenamiento en las técnicas SHADE y SHADE-GD para los conjuntos de datos tabulares. E: Entrenamiento, T:Test, M: Métrica.}}{108}{table.caption.51}\protected@file@percent }
\newlabel{tab:shadevsshadegd}{{22}{108}{Comparación de los resultados del entrenamiento en las técnicas SHADE y SHADE-GD para los conjuntos de datos tabulares. E: Entrenamiento, T:Test, M: Métrica}{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23}{\ignorespaces Comparación de los resultados del entrenamiento en las técnicas SHADE y SHADE-GD para los conjuntos de datos de imágenes. E: Entrenamiento, T:Test, M: Métrica.}}{109}{table.caption.52}\protected@file@percent }
\newlabel{tab:shadevsshadegd_im}{{23}{109}{Comparación de los resultados del entrenamiento en las técnicas SHADE y SHADE-GD para los conjuntos de datos de imágenes. E: Entrenamiento, T:Test, M: Métrica}{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24}{\ignorespaces Comparación de los resultados del entrenamiento en las técnicas SHADE-ILS y SHADE-ILS-GD para los conjuntos de datos tabulares. E: Entrenamiento, T:Test, M: Métrica.}}{109}{table.caption.53}\protected@file@percent }
\newlabel{tab:ilsvsilsgd}{{24}{109}{Comparación de los resultados del entrenamiento en las técnicas SHADE-ILS y SHADE-ILS-GD para los conjuntos de datos tabulares. E: Entrenamiento, T:Test, M: Métrica}{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {25}{\ignorespaces Comparación de los resultados del entrenamiento en las técnicas SHADE-ILS y SHADE-ILS-GD para los conjuntos de datos de imágenes. E: Entrenamiento, T:Test, M: Métrica.}}{110}{table.caption.54}\protected@file@percent }
\newlabel{tab:ilsvsilsgd_im}{{25}{110}{Comparación de los resultados del entrenamiento en las técnicas SHADE-ILS y SHADE-ILS-GD para los conjuntos de datos de imágenes. E: Entrenamiento, T:Test, M: Métrica}{table.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusiones y trabajos futuros}{111}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Objetivos satisfechos}{112}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Trabajos futuros}{113}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Bibliografía}{113}{Item.30}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Apéndice A}{119}{section.12}\protected@file@percent }
\newlabel{sec:apendiceA}{{12}{119}{Apéndice A}{section.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {26}{\ignorespaces Resultados del entrenamiento para el dataset Breast Cancer Winsconsin.}}{119}{table.caption.56}\protected@file@percent }
\newlabel{tab:bcw}{{26}{119}{Resultados del entrenamiento para el dataset Breast Cancer Winsconsin}{table.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {27}{\ignorespaces Resultados del entrenamiento para el dataset Boston Housing Price.}}{119}{table.caption.57}\protected@file@percent }
\newlabel{tab:bhp}{{27}{119}{Resultados del entrenamiento para el dataset Boston Housing Price}{table.caption.57}{}}
\@writefile{lot}{\contentsline {table}{\numberline {28}{\ignorespaces Resultados del entrenamiento para el dataset Wine Quality (Clasificación).}}{119}{table.caption.58}\protected@file@percent }
\newlabel{tab:wqr}{{28}{119}{Resultados del entrenamiento para el dataset Wine Quality (Clasificación)}{table.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {29}{\ignorespaces Resultados del entrenamiento para el dataset Wine Quality (Regresión).}}{119}{table.caption.59}\protected@file@percent }
\newlabel{tab:wqr}{{29}{119}{Resultados del entrenamiento para el dataset Wine Quality (Regresión)}{table.caption.59}{}}
\@writefile{lot}{\contentsline {table}{\numberline {30}{\ignorespaces Resultados del entrenamiento para el dataset MNIST.}}{120}{table.caption.60}\protected@file@percent }
\newlabel{tab:mnist}{{30}{120}{Resultados del entrenamiento para el dataset MNIST}{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {31}{\ignorespaces Resultados del entrenamiento para el dataset F-MNIST}}{120}{table.caption.61}\protected@file@percent }
\newlabel{tab:fmnist}{{31}{120}{Resultados del entrenamiento para el dataset F-MNIST}{table.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {32}{\ignorespaces Resultados del entrenamiento para el dataset CIFAR10.}}{120}{table.caption.62}\protected@file@percent }
\newlabel{tab:cifar10}{{32}{120}{Resultados del entrenamiento para el dataset CIFAR10}{table.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Apéndice B}{121}{section.13}\protected@file@percent }
\newlabel{sec:apendiceB}{{13}{121}{Apéndice B}{section.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {33}{\ignorespaces Tiempo del entrenamiento en segundos para el dataset Boston House Pricing.}}{121}{table.caption.63}\protected@file@percent }
\newlabel{tab:bhp_time}{{33}{121}{Tiempo del entrenamiento en segundos para el dataset Boston House Pricing}{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {34}{\ignorespaces Tiempo del entrenamiento en segundos para el dataset Wine Quality (Regresión).}}{121}{table.caption.64}\protected@file@percent }
\newlabel{tab:wqr_time}{{34}{121}{Tiempo del entrenamiento en segundos para el dataset Wine Quality (Regresión)}{table.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {35}{\ignorespaces Tiempo del entrenamiento en segundos para el dataset MNIST.}}{121}{table.caption.65}\protected@file@percent }
\newlabel{tab:mnist_time}{{35}{121}{Tiempo del entrenamiento en segundos para el dataset MNIST}{table.caption.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {36}{\ignorespaces Tiempo del entrenamiento en segundos para el dataset CIFAR-10.}}{122}{table.caption.66}\protected@file@percent }
\newlabel{tab:cifar_time}{{36}{122}{Tiempo del entrenamiento en segundos para el dataset CIFAR-10}{table.caption.66}{}}
\abx@aux@read@bbl@mdfivesum{6170E783A546EB2C3FFFD61FF9B1821B}
\abx@aux@defaultrefcontext{0}{Ahmadi_2011_NP_Convex}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{176}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{162}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{AutomaticDiff}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ReLuat0}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patternrecog}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecunnDeepForAI}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pso}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{convexSubgrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{L-BFGS-B}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{CauchyGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Curry1944GDNoLin}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dauphin2014SaddlePoints}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Problem3_accel}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{adagrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{BFGS}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stanford_231}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stabilityProblem2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{GoodFellowBook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mhhandbook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ResNets}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heinic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rmsprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{randomforest}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{VanishExplode}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{holland_gen_alg}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{batchnorm}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{beesalgo}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Adam}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{siman}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{163}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NIPS2012_c399862d}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun2015deep}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bottleorig}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{EffBackProp}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lenet5}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{L-BFGS}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{MHtrainingClase}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machine}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NPHardProblem}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shadeils}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{murphy2022probabilistic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Nesterov}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabackprop1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabacknumerical}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Numerical_optimization}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{155}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{diffevbook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{momentumorig}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{174}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelbackprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perceptron}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Sejnowski18}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{diffev}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie3}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{MHoverview}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shade}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nofreelunch}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{divedeeplearning}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yellowfin}{anyt/global//global/global/global}
\gdef \@abspage@last{126}
