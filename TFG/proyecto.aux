\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\abx@aux@refcontext{anyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{mitchell1997machine}
\abx@aux@segm{0}{0}{mitchell1997machine}
\abx@aux@cite{0}{lecun2015deep}
\abx@aux@segm{0}{0}{lecun2015deep}
\abx@aux@cite{0}{NIPS2012_c399862d}
\abx@aux@segm{0}{0}{NIPS2012_c399862d}
\abx@aux@cite{0}{Sejnowski18}
\abx@aux@segm{0}{0}{Sejnowski18}
\abx@aux@cite{0}{lecunnDeepForAI}
\abx@aux@segm{0}{0}{lecunnDeepForAI}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{1}{section.1}\protected@file@percent }
\abx@aux@cite{0}{rumelbackprop}
\abx@aux@segm{0}{0}{rumelbackprop}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{patternrecog}
\abx@aux@segm{0}{0}{patternrecog}
\abx@aux@cite{0}{EffBackProp}
\abx@aux@segm{0}{0}{EffBackProp}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{alternativabacknumerical}
\abx@aux@segm{0}{0}{alternativabacknumerical}
\abx@aux@cite{0}{alternativabackprop1}
\abx@aux@segm{0}{0}{alternativabackprop1}
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.GD}{{1}{2}{Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \cite {GoodFellowBook}}{figure.caption.2}{}}
\abx@aux@cite{0}{NPHardProblem}
\abx@aux@segm{0}{0}{NPHardProblem}
\abx@aux@cite{0}{Problem3_accel}
\abx@aux@segm{0}{0}{Problem3_accel}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivación}{3}{subsection.1.1}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objetivos}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentos previos}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cálculo diferencial}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Teoría de la probabilidad}{8}{subsection.2.2}\protected@file@percent }
\newlabel{def:esprob}{{2.8}{8}{Espacio de probabilidad}{definicion.2.8}{}}
\abx@aux@cite{0}{Curry1944GDNoLin}
\abx@aux@segm{0}{0}{Curry1944GDNoLin}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradiente Descendente}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradiente descendente de Cauchy}{11}{subsection.3.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradiente descendente en el entrenamiento de modelos}{12}{subsection.3.2}\protected@file@percent }
\newlabel{eq:GD}{{2}{12}{Gradiente descendente en el entrenamiento de modelos}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Estrategias de gradiente descendente}{12}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec:estrategias}{{3.2.1}{12}{Estrategias de gradiente descendente}{subsubsection.3.2.1}{}}
\newlabel{eq:SGD}{{3}{13}{Estrategias de gradiente descendente}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}\textit  {Learning rate}}{13}{subsubsection.3.2.2}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualización de cómo afecta el \textit  {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \FN@sf@gobble@opt  {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}}{14}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lr}{{2}{14}{Visualización de cómo afecta el \textit {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \footnote {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Subgradientes}{14}{subsection.3.3}\protected@file@percent }
\newlabel{sec:subgrad}{{3.3}{14}{Subgradientes}{subsection.3.3}{}}
\abx@aux@cite{0}{convexSubgrad}
\abx@aux@segm{0}{0}{convexSubgrad}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Función ReLU y algunas de sus variantes más usadas como funciones de activación.\FN@sf@gobble@opt  {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}}{16}{figure.caption.4}\protected@file@percent }
\newlabel{fig:3.ReLU}{{3}{16}{Función ReLU y algunas de sus variantes más usadas como funciones de activación.\footnote {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}{figure.caption.4}{}}
\newlabel{prop:subgrad}{{3.1}{18}{Existencia de subgradientes}{proposicion.3.1}{}}
\newlabel{eq:des2}{{4}{19}{Subgradientes}{equation.3.4}{}}
\newlabel{proof1:f(x)}{{5}{19}{Subgradientes}{equation.3.5}{}}
\newlabel{proof1:f(y)}{{6}{19}{Subgradientes}{equation.3.6}{}}
\newlabel{proof1:epi}{{7}{19}{Subgradientes}{equation.3.7}{}}
\abx@aux@cite{0}{ReLuat0}
\abx@aux@segm{0}{0}{ReLuat0}
\newlabel{ej:RELUsub}{{3.1}{20}{Subgradiente de la función ReLU}{ejemplo.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convergencia}{20}{subsection.3.4}\protected@file@percent }
\newlabel{sec:convergencia}{{3.4}{20}{Convergencia}{subsection.3.4}{}}
\abx@aux@cite{0}{Ahmadi_2011_NP_Convex}
\abx@aux@segm{0}{0}{Ahmadi_2011_NP_Convex}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Convergencia para BGD}{21}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{proof:gdconvex}{{3.2}{22}{Convergencia para funciones convexas}{teorema.3.2}{}}
\newlabel{eq:gdproof1}{{8}{23}{Convergencia para BGD}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Convergencia para SGD y MBGD}{24}{subsubsection.3.4.2}\protected@file@percent }
\newlabel{eq:martingala}{{9}{25}{Convergencia para SGD y MBGD}{equation.3.9}{}}
\newlabel{teor:sig}{{3.3}{25}{Teorema de Robbins-Siegmund}{teorema.3.3}{}}
\newlabel{eq:sig1}{{15}{27}{Convergencia para SGD y MBGD}{equation.3.15}{}}
\newlabel{teor:convsgd}{{3.4}{27}{Convergencia de algoritmos SGD}{teorema.3.4}{}}
\newlabel{eq:convsgd1}{{16}{27}{Convergencia de algoritmos SGD}{equation.3.16}{}}
\newlabel{eq:convsgd2}{{17}{27}{Convergencia de algoritmos SGD}{equation.3.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Problemas en la convergencia}{28}{subsubsection.3.4.3}\protected@file@percent }
\abx@aux@cite{0}{dauphin2014SaddlePoints}
\abx@aux@segm{0}{0}{dauphin2014SaddlePoints}
\@writefile{toc}{\contentsline {section}{\numberline {4}BP}{29}{section.4}\protected@file@percent }
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diferenciación automática}{30}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Diferenciación hacia delante vs hacia atrás en un MLP}{31}{subsection.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Diferenciación hacia delante}}{33}{algorithm.1}\protected@file@percent }
\newlabel{alg:fowdiff}{{1}{33}{Diferenciación hacia delante}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Diferenciación en modo reverso}}{33}{algorithm.2}\protected@file@percent }
\newlabel{alg:backdif}{{2}{33}{Diferenciación en modo reverso}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}BP para MLP}{34}{subsection.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces BP para MLP con k capas}}{35}{algorithm.3}\protected@file@percent }
\newlabel{alg:BPMLPk}{{3}{35}{BP para MLP con k capas}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Capa no-lineal}{36}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Capa Cross Entropy}{36}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Capa lineal}{37}{subsubsection.4.3.3}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representación del grafo dirigido acíclico (imagen obtenida de \blx@tocontentsinit {0}\cite {murphy2022probabilistic})}}{39}{figure.caption.5}\protected@file@percent }
\newlabel{fig:def.grafo}{{4}{39}{Representación del grafo dirigido acíclico (imagen obtenida de \cite {murphy2022probabilistic})}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Grafos computacionales}{39}{subsubsection.4.3.4}\protected@file@percent }
\abx@aux@cite{0}{VanishExplode}
\abx@aux@segm{0}{0}{VanishExplode}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Problemas con el cálculo del gradiente}{40}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Desvanecimiento y explosión del gradiente}{40}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:desvyexpl}{{4.4.1}{40}{Desvanecimiento y explosión del gradiente}{subsubsection.4.4.1}{}}
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Inicialización de los pesos}{41}{subsubsection.4.4.2}\protected@file@percent }
\newlabel{sec:inipesos}{{4.4.2}{41}{Inicialización de los pesos}{subsubsection.4.4.2}{}}
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\abx@aux@cite{0}{heinic}
\abx@aux@segm{0}{0}{heinic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Parte informática: enfoque clásico vs técnicas metaheurísticas}{43}{part.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{ResNets}
\abx@aux@segm{0}{0}{ResNets}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\abx@aux@cite{0}{MHoverview}
\abx@aux@segm{0}{0}{MHoverview}
\@writefile{toc}{\contentsline {section}{\numberline {5}Introducción}{44}{section.5}\protected@file@percent }
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivación}{45}{subsection.5.1}\protected@file@percent }
\newlabel{sec:motinfo}{{5.1}{45}{Motivación}{subsection.5.1}{}}
\abx@aux@cite{0}{y}
\abx@aux@segm{0}{0}{y}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Objetivos}{46}{subsection.5.2}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{mhhandbook}
\abx@aux@segm{0}{0}{mhhandbook}
\abx@aux@cite{0}{diffevbook}
\abx@aux@segm{0}{0}{diffevbook}
\abx@aux@cite{0}{numerical_optimization}
\abx@aux@segm{0}{0}{numerical_optimization}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{perceptron}
\abx@aux@segm{0}{0}{perceptron}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\@writefile{toc}{\contentsline {section}{\numberline {6}Fundamentos previos}{48}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Redes neuronales y aprendizaje profundo}{48}{subsection.6.1}\protected@file@percent }
\newlabel{sec:profundo}{{6.1}{48}{Redes neuronales y aprendizaje profundo}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Red neuronal}{48}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Esquema del modelo de una neurona con tres conexiones de entrada. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{49}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Perceptron}{{5}{49}{Esquema del modelo de una neurona con tres conexiones de entrada. Obtenida de \cite {stanford_231}}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Aprendizaje profundo y redes neuronales profundas}{49}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}MLP}{49}{subsubsection.6.1.3}\protected@file@percent }
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{lenet5}
\abx@aux@segm{0}{0}{lenet5}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Red neuronal de tres capas (sin contar la capa de entrada) con dos capas ocultas de cuatro neuronas cada una y una capa de salida con una neurona. Destacar que no hay conexiones de una neurona con varias capas. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{50}{figure.caption.7}\protected@file@percent }
\newlabel{fig:NeuralNet}{{6}{50}{Red neuronal de tres capas (sin contar la capa de entrada) con dos capas ocultas de cuatro neuronas cada una y una capa de salida con una neurona. Destacar que no hay conexiones de una neurona con varias capas. Obtenida de \cite {stanford_231}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ConvNets}{50}{subsection.6.2}\protected@file@percent }
\newlabel{sec:convnets}{{6.2}{50}{ConvNets}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Operación de convolución}{50}{subsubsection.6.2.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Capa Convolucional}{51}{subsubsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convolución 2D sin voltear el filtro (\textit  {cross-relation}). Obtenida de \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{52}{figure.caption.8}\protected@file@percent }
\newlabel{fig:3.Conv}{{7}{52}{Convolución 2D sin voltear el filtro (\textit {cross-relation}). Obtenida de \cite {GoodFellowBook}}{figure.caption.8}{}}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ilustración de la disposición espacial. El tamaño de la entrada es $W=5$ (vector gris) y el del filtro $F=3$ (vector verde), sin usar padding $P=1$. En el ejemplo de la izquierda se usa \textit  {stride} $S=1$, mientras que en el de la derecha se usa $S=2$, obteniendo tamaños de salida de 5 y 3, respectivamente. Estos tamaños se pueden calcular según la fórmula \ref {eq:output}. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{53}{figure.caption.9}\protected@file@percent }
\newlabel{fig:stride}{{8}{53}{Ilustración de la disposición espacial. El tamaño de la entrada es $W=5$ (vector gris) y el del filtro $F=3$ (vector verde), sin usar padding $P=1$. En el ejemplo de la izquierda se usa \textit {stride} $S=1$, mientras que en el de la derecha se usa $S=2$, obteniendo tamaños de salida de 5 y 3, respectivamente. Estos tamaños se pueden calcular según la fórmula \ref {eq:output}. Obtenida de \cite {stanford_231}}{figure.caption.9}{}}
\newlabel{eq:output}{{18}{53}{Capa Convolucional}{equation.6.18}{}}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{stanford_231}
\abx@aux@segm{0}{0}{stanford_231}
\abx@aux@cite{0}{batchnorm}
\abx@aux@segm{0}{0}{batchnorm}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Capa de pooling con un filtro de tamaño $2 \times 2$ y stride 2. Obtenida de \blx@tocontentsinit {0}\cite {stanford_231}}}{54}{figure.caption.10}\protected@file@percent }
\newlabel{fig:pooling}{{9}{54}{Capa de pooling con un filtro de tamaño $2 \times 2$ y stride 2. Obtenida de \cite {stanford_231}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Capa Pooling}{54}{subsubsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Capa \textit  {Batch Normalization}}{54}{subsubsection.6.2.4}\protected@file@percent }
\abx@aux@cite{0}{ResNets}
\abx@aux@segm{0}{0}{ResNets}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{divedeeplearning}
\abx@aux@segm{0}{0}{divedeeplearning}
\abx@aux@cite{0}{bottleorig}
\abx@aux@segm{0}{0}{bottleorig}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.5}Capa FC}{55}{subsubsection.6.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}ResNets}{55}{subsection.6.3}\protected@file@percent }
\newlabel{sec:resnets}{{6.3}{55}{ResNets}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Bloques residuales}{55}{subsubsection.6.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces En un bloque convolucional estándar (izquierda), la parte dentro de la línea de puntos debe aprender directamente la función $f(x)$. En un bloque residual (derecha), la parte dentro de la línea de puntos debe aprender la función residual $g(x)=f(x)-x$, haciendo que la función identidad $f(x)=x$ sea más fácil de aprender. Obtenida de \blx@tocontentsinit {0}\cite {divedeeplearning}}}{56}{figure.caption.11}\protected@file@percent }
\newlabel{fig:resblock}{{10}{56}{En un bloque convolucional estándar (izquierda), la parte dentro de la línea de puntos debe aprender directamente la función $f(x)$. En un bloque residual (derecha), la parte dentro de la línea de puntos debe aprender la función residual $g(x)=f(x)-x$, haciendo que la función identidad $f(x)=x$ sea más fácil de aprender. Obtenida de \cite {divedeeplearning}}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Bloque residual donde la entrada tiene la misma dimensión que la salida (izquierda), y bloque residual donde se transforma el tamaño de la entrada a través de convoluciones $1\times 1$ para que tengan el mismo tamaño (derecha). Obtenida de \blx@tocontentsinit {0}\cite {divedeeplearning}}}{56}{figure.caption.12}\protected@file@percent }
\newlabel{fig:resblock1x1}{{11}{56}{Bloque residual donde la entrada tiene la misma dimensión que la salida (izquierda), y bloque residual donde se transforma el tamaño de la entrada a través de convoluciones $1\times 1$ para que tengan el mismo tamaño (derecha). Obtenida de \cite {divedeeplearning}}{figure.caption.12}{}}
\abx@aux@cite{0}{leslie1}
\abx@aux@segm{0}{0}{leslie1}
\abx@aux@cite{0}{leslie2}
\abx@aux@segm{0}{0}{leslie2}
\abx@aux@cite{0}{leslie3}
\abx@aux@segm{0}{0}{leslie3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Convoluciones 1x1}{57}{subsubsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Política de ciclos de Leslie}{57}{subsection.6.4}\protected@file@percent }
\abx@aux@cite{0}{momentumorig}
\abx@aux@segm{0}{0}{momentumorig}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{Nesterov}
\abx@aux@segm{0}{0}{Nesterov}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Optimizadores de gradiente descendente}{58}{subsection.6.5}\protected@file@percent }
\newlabel{sec:gd}{{6.5}{58}{Optimizadores de gradiente descendente}{subsection.6.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}NAG}{58}{subsubsection.6.5.1}\protected@file@percent }
\newlabel{fig:momentum}{{\caption@xref {fig:momentum}{ on input line 229}}{59}{NAG}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparación entre el algoritmo de gradiente descendente sin usar el método del momento (izquierda) y usándolo (derecha). Vemos que en la figura de la derecha se diferencia la dirección del gradiente (flechas negras) y el camino seguido por el algoritmo en rojo. Imágenes obtenidas de \blx@tocontentsinit {0}\cite {GoodFellowBook}.}}{59}{figure.caption.13}\protected@file@percent }
\abx@aux@cite{0}{rmsprop}
\abx@aux@segm{0}{0}{rmsprop}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparación entre los métodos del momento original (vectores azules) y el momento de Nesterov. En este último, primero realizamos un salto grande en la dirección del gradiente acumulado (vector marrrón) para luego medir el gradiente de la posición al acabar el salto y realizar una corrección (vector rojo). La flecha verde indica la posición final corregida donde acaba una iteración del método NAG. Obtenida de \url  {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}}{60}{figure.caption.14}\protected@file@percent }
\newlabel{fig:resblock1x1}{{13}{60}{Comparación entre los métodos del momento original (vectores azules) y el momento de Nesterov. En este último, primero realizamos un salto grande en la dirección del gradiente acumulado (vector marrrón) para luego medir el gradiente de la posición al acabar el salto y realizar una corrección (vector rojo). La flecha verde indica la posición final corregida donde acaba una iteración del método NAG. Obtenida de \url {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}RMSProp}{60}{subsubsection.6.5.2}\protected@file@percent }
\abx@aux@cite{0}{adagrad}
\abx@aux@segm{0}{0}{adagrad}
\abx@aux@cite{0}{Adam}
\abx@aux@segm{0}{0}{Adam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Adam}{61}{subsubsection.6.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Metaheurísticas}{61}{subsection.6.6}\protected@file@percent }
\newlabel{sec:mh}{{6.6}{61}{Metaheurísticas}{subsection.6.6}{}}
\abx@aux@cite{0}{holland_gen_alg}
\abx@aux@segm{0}{0}{holland_gen_alg}
\abx@aux@cite{0}{holland_gen_alg}
\abx@aux@segm{0}{0}{holland_gen_alg}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Famosa imagen esquemática del operador de cruce en un punto para dos vectores binarios de soluciones (izquierda), comparando el proceso con la recombinación genética de cromosomas (derecha). Obtenida de \blx@tocontentsinit {0}\cite {holland_gen_alg}}}{62}{figure.caption.15}\protected@file@percent }
\newlabel{fig:cruce_mh}{{14}{62}{Famosa imagen esquemática del operador de cruce en un punto para dos vectores binarios de soluciones (izquierda), comparando el proceso con la recombinación genética de cromosomas (derecha). Obtenida de \cite {holland_gen_alg}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}Metaheurísticas basadas en poblaciones}{62}{subsubsection.6.6.1}\protected@file@percent }
\abx@aux@cite{0}{diffev}
\abx@aux@segm{0}{0}{diffev}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Esquema de la ejecución de un algoritmo basado en poblaciones donde se observan las etapas de cada generación. Obtenida de \url  {https://blogs.imf-formacion.com/blog/tecnologia/}}}{63}{figure.caption.16}\protected@file@percent }
\newlabel{fig:gen_alg}{{15}{63}{Esquema de la ejecución de un algoritmo basado en poblaciones donde se observan las etapas de cada generación. Obtenida de \url {https://blogs.imf-formacion.com/blog/tecnologia/}}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Differential Evolution}{63}{subsubsection.6.6.2}\protected@file@percent }
\abx@aux@cite{0}{L-BFGS-B}
\abx@aux@segm{0}{0}{L-BFGS-B}
\abx@aux@cite{0}{BFGS}
\abx@aux@segm{0}{0}{BFGS}
\abx@aux@cite{0}{L-BFGS}
\abx@aux@segm{0}{0}{L-BFGS}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}L-BFGS-B}{64}{subsubsection.6.6.3}\protected@file@percent }
\abx@aux@cite{0}{shade}
\abx@aux@segm{0}{0}{shade}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Esquema general de DE}}{65}{algorithm.4}\protected@file@percent }
\newlabel{alg:de}{{4}{65}{Esquema general de DE}{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.4}SHADE}{65}{subsubsection.6.6.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Algoritmo SHADE}}{66}{algorithm.5}\protected@file@percent }
\newlabel{alg:shade}{{5}{66}{Algoritmo SHADE}{algorithm.5}{}}
\abx@aux@cite{0}{shadeils}
\abx@aux@segm{0}{0}{shadeils}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.5}Algoritmos meméticos}{67}{subsubsection.6.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.6}SHADE-ILS}{67}{subsubsection.6.6.6}\protected@file@percent }
\newlabel{sec:shade-ils}{{6.6.6}{67}{SHADE-ILS}{subsubsection.6.6.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Algoritmo SHADE-ILS}}{68}{algorithm.6}\protected@file@percent }
\newlabel{alg:shade-ils}{{6}{68}{Algoritmo SHADE-ILS}{algorithm.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Resultados de la web SCOPUS para la búsqueda TITLE-ABS-KEY ( deep AND learning AND training ), muestra el número de artículos por año.}}{69}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scopus_deep}{{16}{69}{Resultados de la web SCOPUS para la búsqueda TITLE-ABS-KEY ( deep AND learning AND training ), muestra el número de artículos por año}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Estado del arte}{69}{section.7}\protected@file@percent }
\newlabel{fig:resEdA}{{\caption@xref {fig:resEdA}{ on input line 40}}{70}{Estado del arte}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Artículos publicados por año para el entrenamiento de modelos de aprendizaje profundo con metaheurísticas (arriba) y gradiente descendente (debajo) según las búsquedas correspondientes.}}{70}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Gradiente descendente y optimizadores}{71}{subsection.7.1}\protected@file@percent }
\abx@aux@cite{0}{yellowfin}
\abx@aux@segm{0}{0}{yellowfin}
\abx@aux@cite{0}{162}
\abx@aux@segm{0}{0}{162}
\abx@aux@cite{0}{beesalgo}
\abx@aux@segm{0}{0}{beesalgo}
\abx@aux@cite{0}{155}
\abx@aux@segm{0}{0}{155}
\abx@aux@cite{0}{163}
\abx@aux@segm{0}{0}{163}
\abx@aux@cite{0}{pso}
\abx@aux@segm{0}{0}{pso}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Tabla con los datasets utilizados con sus respectivos modelos en la experimentación de la comparativa (enlace)}}{72}{table.caption.19}\protected@file@percent }
\newlabel{table:exp}{{1}{72}{Tabla con los datasets utilizados con sus respectivos modelos en la experimentación de la comparativa (enlace)}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Metaheurísticas en el entrenamiento de modelos}{72}{subsection.7.2}\protected@file@percent }
\abx@aux@cite{0}{174}
\abx@aux@segm{0}{0}{174}
\abx@aux@cite{0}{176}
\abx@aux@segm{0}{0}{176}
\abx@aux@cite{0}{siman}
\abx@aux@segm{0}{0}{siman}
\abx@aux@cite{0}{MHtrainingClase}
\abx@aux@segm{0}{0}{MHtrainingClase}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}SHADE-ILS}{74}{subsubsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Experimentación}{74}{section.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas.}}{75}{table.caption.20}\protected@file@percent }
\newlabel{table:exp}{{2}{75}{Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas}{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Detalles de los modelos MLP}}{75}{table.caption.21}\protected@file@percent }
\newlabel{tab:MLPmod}{{3}{75}{Detalles de los modelos MLP}{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Modelos}{75}{subsection.8.1}\protected@file@percent }
\abx@aux@cite{0}{lenet5}
\abx@aux@segm{0}{0}{lenet5}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{76}{table.caption.22}\protected@file@percent }
\newlabel{table:lenet5}{{4}{76}{Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.22}{}}
\abx@aux@cite{0}{bottleorig}
\abx@aux@segm{0}{0}{bottleorig}
\abx@aux@cite{0}{bottlegoogle}
\abx@aux@segm{0}{0}{bottlegoogle}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Datasets}{77}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Tabulares}{77}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{78}{table.caption.23}\protected@file@percent }
\newlabel{table:resnet57}{{5}{78}{Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{78}{table.caption.24}\protected@file@percent }
\newlabel{table:resnet15}{{6}{78}{Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Imágenes}{79}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Otras decisiones}{79}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Optimizadores}{80}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Metaheurísticas}{81}{subsection.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Bibliografía}{82}{subsection.8.5}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{22DA2858B41DA63BF8CA53AB7CFC16AC}
\abx@aux@defaultrefcontext{0}{Ahmadi_2011_NP_Convex}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{176}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{162}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{AutomaticDiff}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ReLuat0}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patternrecog}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecunnDeepForAI}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pso}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{convexSubgrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{L-BFGS-B}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{CauchyGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Curry1944GDNoLin}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dauphin2014SaddlePoints}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Problem3_accel}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{adagrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{BFGS}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stanford_231}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stabilityProblem2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{GoodFellowBook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mhhandbook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ResNets}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heinic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rmsprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{VanishExplode}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{holland_gen_alg}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{batchnorm}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{beesalgo}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Adam}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{siman}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{163}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NIPS2012_c399862d}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun2015deep}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bottleorig}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{EffBackProp}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lenet5}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{L-BFGS}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{MHtrainingClase}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machine}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NPHardProblem}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shadeils}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{murphy2022probabilistic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Nesterov}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabackprop1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabacknumerical}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{155}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{diffevbook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{momentumorig}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{174}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelbackprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perceptron}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Sejnowski18}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{diffev}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leslie3}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bottlegoogle}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{MHoverview}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shade}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{divedeeplearning}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yellowfin}{anyt/global//global/global/global}
\gdef \@abspage@last{91}
