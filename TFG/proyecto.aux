\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\abx@aux@refcontext{anyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Parte matemática: gradiente descendente y \textit  {backpropagation}}{1}{part.1}\protected@file@percent }
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{mitchell1997machine}
\abx@aux@segm{0}{0}{mitchell1997machine}
\abx@aux@cite{0}{lecun2015deep}
\abx@aux@segm{0}{0}{lecun2015deep}
\abx@aux@cite{0}{NIPS2012_c399862d}
\abx@aux@segm{0}{0}{NIPS2012_c399862d}
\abx@aux@cite{0}{Sejnowski18}
\abx@aux@segm{0}{0}{Sejnowski18}
\abx@aux@cite{0}{lecunnDeepForAI}
\abx@aux@segm{0}{0}{lecunnDeepForAI}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{2}{section.1}\protected@file@percent }
\abx@aux@cite{0}{rumelbackprop}
\abx@aux@segm{0}{0}{rumelbackprop}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{patternrecog}
\abx@aux@segm{0}{0}{patternrecog}
\abx@aux@cite{0}{EffBackProp}
\abx@aux@segm{0}{0}{EffBackProp}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\abx@aux@cite{0}{alternativabacknumerical}
\abx@aux@segm{0}{0}{alternativabacknumerical}
\abx@aux@cite{0}{alternativabackprop1}
\abx@aux@segm{0}{0}{alternativabackprop1}
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \blx@tocontentsinit {0}\cite {GoodFellowBook}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:1.GD}{{1}{3}{Esquematización de la estrategia de descenso del gradiente en un modelo con un solo parámetro x. El eje horizontal representa los valores que toma éste y el vertical representa el error del modelo en función de x. Imagen obtenida y traducida del libro \cite {GoodFellowBook}}{figure.1}{}}
\abx@aux@cite{0}{NPHardProblem}
\abx@aux@segm{0}{0}{NPHardProblem}
\abx@aux@cite{0}{Problem3_accel}
\abx@aux@segm{0}{0}{Problem3_accel}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivación}{4}{subsection.1.1}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objetivos}{5}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Fundamentos previos}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cálculo diferencial}{5}{subsection.2.1}\protected@file@percent }
\abx@aux@cite{0}{Curry1944GDNoLin}
\abx@aux@segm{0}{0}{Curry1944GDNoLin}
\abx@aux@cite{0}{CauchyGD}
\abx@aux@segm{0}{0}{CauchyGD}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradiente Descendente}{9}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradiente descendente de Cauchy}{10}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradiente descendente en el entrenamiento de modelos}{10}{subsection.3.2}\protected@file@percent }
\newlabel{eq:GD}{{1}{10}{Gradiente descendente en el entrenamiento de modelos}{equation.3.1}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Estrategias de gradiente descendente}{11}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{sec:estrategias}{{3.2.1}{11}{Estrategias de gradiente descendente}{subsubsection.3.2.1}{}}
\abx@aux@cite{0}{GoodFellowBook}
\abx@aux@segm{0}{0}{GoodFellowBook}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualización de cómo afecta el \textit  {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \FN@sf@gobble@opt  {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}}{12}{figure.2}\protected@file@percent }
\newlabel{fig:lr}{{2}{12}{Visualización de cómo afecta el \textit {learning rate} según su adecuación al problema. Imagen obtenida del curso de Caltech \footnote {https://home.work.caltech.edu/slides/slides09.pdf}, tema 9 diapositiva 21}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}\textit  {Learning rate}}{12}{subsubsection.3.2.2}\protected@file@percent }
\abx@aux@cite{0}{convexSubgrad}
\abx@aux@segm{0}{0}{convexSubgrad}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Subgradientes}{13}{subsection.3.3}\protected@file@percent }
\newlabel{sec:subgrad}{{3.3}{13}{Subgradientes}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Función ReLU y algunas de sus variantes más usadas como funciones de activación.\FN@sf@gobble@opt  {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}}{14}{figure.3}\protected@file@percent }
\newlabel{fig:3.ReLU}{{3}{14}{Función ReLU y algunas de sus variantes más usadas como funciones de activación.\footnote {https://www.researchgate.net/publication/319438080\_A\_novel\_softplus\_linear\_unit\_for\_deep\_convolutional\_neural\_networks}}{figure.3}{}}
\newlabel{prop:subgrad}{{3.1}{16}{Existencia de subgradientes}{proposicion.3.1}{}}
\newlabel{eq:des2}{{2}{17}{Subgradientes}{equation.3.2}{}}
\newlabel{proof1:f(x)}{{3}{17}{Subgradientes}{equation.3.3}{}}
\newlabel{proof1:f(y)}{{4}{17}{Subgradientes}{equation.3.4}{}}
\abx@aux@cite{0}{ReLuat0}
\abx@aux@segm{0}{0}{ReLuat0}
\newlabel{proof1:epi}{{5}{18}{Subgradientes}{equation.3.5}{}}
\newlabel{ej:RELUsub}{{3.1}{18}{Subgradiente de la función ReLU}{ejemplo.3.1}{}}
\abx@aux@cite{0}{Ahmadi_2011_NP_Convex}
\abx@aux@segm{0}{0}{Ahmadi_2011_NP_Convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convergencia}{19}{subsection.3.4}\protected@file@percent }
\newlabel{sec:convergencia}{{3.4}{19}{Convergencia}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Resultados teóricos para la convergencia del gradiente descendente}{19}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{proof:gdconvex}{{3.2}{20}{Convergencia para funciones convexas}{teorema.3.2}{}}
\newlabel{eq:gdproof1}{{6}{21}{Resultados teóricos para la convergencia del gradiente descendente}{equation.3.6}{}}
\abx@aux@cite{0}{RobbinSiegmund}
\abx@aux@segm{0}{0}{RobbinSiegmund}
\abx@aux@cite{0}{RobbinSiegmundtoSGD}
\abx@aux@segm{0}{0}{RobbinSiegmundtoSGD}
\newlabel{prop:convsgd}{{3.2}{22}{Convergencia para SGD}{proposicion.3.2}{}}
\abx@aux@cite{0}{dauphin2014SaddlePoints}
\abx@aux@segm{0}{0}{dauphin2014SaddlePoints}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Problemas en la convergencia}{23}{subsubsection.3.4.2}\protected@file@percent }
\abx@aux@cite{0}{AutomaticDiff}
\abx@aux@segm{0}{0}{AutomaticDiff}
\@writefile{toc}{\contentsline {section}{\numberline {4}BP}{24}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diferenciación automática}{24}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Diferenciación hacia delante vs hacia atrás en un MLP}{25}{subsection.4.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Diferenciación hacia delante}}{27}{algorithm.1}\protected@file@percent }
\newlabel{alg:fowdiff}{{1}{27}{Diferenciación hacia delante vs hacia atrás en un MLP}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Diferenciación en modo reverso}}{28}{algorithm.2}\protected@file@percent }
\newlabel{alg:backdif}{{2}{28}{Diferenciación hacia delante vs hacia atrás en un MLP}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}BP para MLP}{28}{subsection.4.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces BP para MLP con k capas}}{30}{algorithm.3}\protected@file@percent }
\newlabel{alg:BPMLPk}{{3}{30}{BP para MLP}{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Capa no-lineal}{30}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Capa Cross Entropy}{31}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Capa lineal}{31}{subsubsection.4.3.3}\protected@file@percent }
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representación del grafodirigido acíclico (imagen obtenida de \blx@tocontentsinit {0}\cite {murphy2022probabilistic})}}{33}{figure.4}\protected@file@percent }
\newlabel{fig:def.grafo}{{4}{33}{Representación del grafodirigido acíclico (imagen obtenida de \cite {murphy2022probabilistic})}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Grafos computacionales}{33}{subsubsection.4.3.4}\protected@file@percent }
\abx@aux@cite{0}{VanishExplode}
\abx@aux@segm{0}{0}{VanishExplode}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Problemas con el cálculo del gradiente}{34}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Desvanecimiento y explosión del gradiente}{34}{subsubsection.4.4.1}\protected@file@percent }
\newlabel{sec:desvyexpl}{{4.4.1}{34}{Desvanecimiento y explosión del gradiente}{subsubsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Inicialización de los pesos}{35}{subsubsection.4.4.2}\protected@file@percent }
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\abx@aux@cite{0}{stabilityProblem2}
\abx@aux@segm{0}{0}{stabilityProblem2}
\abx@aux@cite{0}{heinic}
\abx@aux@segm{0}{0}{heinic}
\abx@aux@cite{0}{murphy2022probabilistic}
\abx@aux@segm{0}{0}{murphy2022probabilistic}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Parte informática: enfoque clásico vs técnicas metaheurísticas}{38}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Introducción}{39}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivación}{40}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Objetivos}{41}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas.}}{42}{table.1}\protected@file@percent }
\newlabel{table:exp}{{1}{42}{Resumen de la experimentación. BHP: Boston Housing Price, BCW: Breast Cancer Winsconsin, WQ: Wine Quality. R: regresión, C: clasificación. En el caso de MLP, en la fila modelo se indica el número de capas ocultas}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experimentación}{42}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Modelos}{42}{subsection.6.1}\protected@file@percent }
\abx@aux@cite{0}{lenet5}
\abx@aux@segm{0}{0}{lenet5}
\abx@aux@cite{0}{ResNets}
\abx@aux@segm{0}{0}{ResNets}
\abx@aux@cite{0}{bottleorig}
\abx@aux@segm{0}{0}{bottleorig}
\abx@aux@cite{0}{bottlegoogle}
\abx@aux@segm{0}{0}{bottlegoogle}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{44}{table.2}\protected@file@percent }
\newlabel{table:lenet5}{{2}{44}{Topología de LeNet5 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Datasets}{44}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Tabulares}{44}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{45}{table.3}\protected@file@percent }
\newlabel{table:resnet15}{{3}{45}{Topología de ResNet15 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa.}}{45}{table.4}\protected@file@percent }
\newlabel{table:resnet57}{{4}{45}{Topología de ResNet57 para imágenes 32x32 con un canal de entrada. Las columnas dimensión y canales hacen referencia a la salida de la capa}{table.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Imágenes}{46}{subsubsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Otras decisiones}{46}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Optimizadores}{47}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Metaheurísticas}{48}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Bibliografía}{48}{subsection.6.5}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{92C498CF08731FB4E306E0FB83BB1C69}
\abx@aux@defaultrefcontext{0}{Ahmadi_2011_NP_Convex}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{AutomaticDiff}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ReLuat0}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patternrecog}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecunnDeepForAI}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{convexSubgrad}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{CauchyGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Curry1944GDNoLin}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dauphin2014SaddlePoints}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Problem3_accel}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{RobbinSiegmund}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{stabilityProblem2}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{GoodFellowBook}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{RobbinSiegmundtoSGD}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ResNets}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heinic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{VanishExplode}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NIPS2012_c399862d}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun2015deep}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bottleorig}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{EffBackProp}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lenet5}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitchell1997machine}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NPHardProblem}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{murphy2022probabilistic}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabackprop1}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alternativabacknumerical}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelbackprop}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Sejnowski18}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bottlegoogle}{anyt/global//global/global/global}
\gdef \@abspage@last{54}
